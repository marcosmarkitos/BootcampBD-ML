{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy es otra librería open source en Python para NLP.\n",
    "\n",
    "La diferencia fundamental entre NLTK y spaCy es que el primero es muy cómodo para aprender e iniciarse mientras que el segundo está pensado para productizar.\n",
    "\n",
    "Gensim, otra librería, la veremos más adelante cuando estudiemos Topic Modeling y Word Embeddings.\n",
    "\n",
    "Documentación de spaCy: https://spacy.io/\n",
    "\n",
    "La filosofía de trabajo en spaCy es que si existen una serie de algoritmos que solucionan un problema, dar la solución al problema con un único. Además, su funcionamiento se basa en la construcción de pipelines.\n",
    "\n",
    "\n",
    "<img src=https://i.imgur.com/nD7ut2U.jpg>\n",
    "\n",
    "¿Qué capacidades (modelos) linguísticas nos ofrece spaCy?\n",
    "\n",
    "<img src=https://i.imgur.com/lGcL6lx.jpg>\n",
    "\n",
    "Es decir, de spaCy podremos sacar siempre que queramos tokens, pos tags, árboles de dependencia, o entidades nombradas. Incluye también modelos de word embeddings (que veremos con más detalle en sesiones posteriores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://spacy.io/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg width=550px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de spaCy\n",
    "\n",
    "Modelos pre-entrenados para diferentes idiomas y con diferentes corpus. Pueden ser descargados de diferentes maneras, tanto descarga directa, como con pip.\n",
    "\n",
    "Link: https://spacy.io/usage/models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz --no-deps\n",
    "# pip install https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.3.0/es_core_news_sm-2.3.0.tar.gz#egg=es_core_news_sm==2.3.0 --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy download es_core_news_sm\n",
    "# !pip install -U spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import es_core_news_sm\n",
    "# import en_core_web_sm\n",
    "\n",
    "# nlp_es = es_core_news_sm.load()\n",
    "# nlp_en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Modelo \"pequeño\" entrenado con noticias en castellano\n",
    "# https://spacy.io/models/es\n",
    "nlp_es = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Modelo \"pequeño\" entrenado con página\n",
    "# https://spacy.io/models/en\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline del modelo por defecto\n",
    "nlp_es.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines en spaCy:\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "<img src=https://d33wubrfki0l68.cloudfront.net/16b2ccafeefd6d547171afa23f9ac62f159e353d/48b91/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Mi nombre es Carlos y vivo en Madrid. Hoy es lunes 29 de junio'\n",
    "doc = nlp_es(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(doc.sents):\n",
    "    print('Frase {0:5}{1:5}'.format(str(idx), sent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, token in enumerate(doc):\n",
    "    print('Token {0:5}{1:5}'.format(str(idx), token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}{2:5}'.format('Token', 'Shape', 'is_alpha'))\n",
    "for token in doc:\n",
    "    print('{0:10}{1:10}{2:5}'.format(token.text, token.shape_, str(token.is_alpha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '  Mi   nombre es Carlos y vivo en Madrid. Hoy es lunes 29 de junio  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separar por espacios\n",
    "text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar stop words\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "print(list(STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text_1 = 'Soy una frase de ejemplo de la cual vamos a eliminar los stopwords'\n",
    "[word for word in ex_text_1.lower().split() if word not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debate: ¿Pensáis que, en general, se deben filtrar los stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex_text_2 = 'No me gusta esta canción'\n",
    "[word for word in ex_text_2.lower().split() if word not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging\n",
    "\n",
    "El PoS Tagging es una técnica **fundamental** en NLP que consiste en etiquetar cada palabra de un documento en su correspondiente categría gramatical.\n",
    "\n",
    "<img src=https://blog.aaronccwong.com/assets/images/bigram-hmm/pos-title.jpg width=650px>\n",
    "\n",
    "¿Utilidad? Muchísima:\n",
    "\n",
    "- Posibilidad de encontrar los adjetivos / sustantivos /adverbios más comunes\n",
    "- _Ayuda_ a Lemmatizers al desambiguar entre palabras\n",
    "- Grafos en los que los nodos son entidades y verbos. Posibilidad de analizar relaciones entre entidades -> Pintar ejemplo\n",
    "- Posibles features (la distribución de categorías no siempre es homogénea en función del contexto)\n",
    "\n",
    "Podríamos hacer un algoritmo que mirara verbos entre otras entidades, como nombres o adjetivos, y que fuera el verbo quien decidiera que relación tienen esas entidades. Eso se suele hacer para crear bases de datos de grafos, donde cada nodo es una entidad, y entre entidades hay relaciones.\n",
    "\n",
    "Veamos algún ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atributos (https://spacy.io/api/token#attributes):\n",
    "- pos_: tipo de palabra (sustantivo, verbo, adjetivo, etc)\n",
    "- tag_: tipo de palabra especificando más atributos\n",
    "- dep_: relación de dependencia sintáctica\n",
    "\n",
    "Explicación de los términos:\n",
    "https://github.com/explosion/spaCy/blob/master/spacy/glossary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}'.format('Token', 'pos'))\n",
    "for idx, token in enumerate(doc):\n",
    "    print('{0:10}{1:10}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}'.format('Token', 'tag'))\n",
    "for idx, token in enumerate(doc):\n",
    "    print('{0:10}{1:10}'.format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}{2:10}'.format('Token', 'dep', 'Meaning'))\n",
    "for idx, token in enumerate(doc):\n",
    "    print('{0:10}{1:10}{2:10}'.format(token.text, token.dep_, str(spacy.explain(token.dep_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencia sintáctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance':100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de entidades nombradas (NER)\n",
    "\n",
    "El reconocimiento de entidades nombradas (Named Entity Recognition, NER, por sus siglas en inglés) trata de detectar posibles entidades nombradas y, posteriormente, clasificarlas entre un conjunto de categorías predefinidas.\n",
    "\n",
    "Ejemplos de entidades nombradas: nombres de personas, lugares, cantidades, empresas...\n",
    "\n",
    "Pero, ¿qué es exactamente una _entidad nombrada_? Según definió Saul Kripke * (filósofo y lógico) son - o deberían ser - todas aquellas entidades para las cuales existe uno - o más de uno - designador rígido. Es decir, dicha palabra / expresión se refiere a la misma cosa / entidad con independencia del contexto.\n",
    "\n",
    "<img src=https://hyscore.io/wp-content/uploads/2019/03/illustration_named_entity_recognition-1024x486-1.jpg width=700px>\n",
    "\n",
    "El rendimiento de los NER varía mucho en función del idioma en el que han sido entrenados. El rendimiento que se comienza a obtener (debido principalmente al uso de modelos de embeddings contextuales) supera al de un ser humano.\n",
    "\n",
    "Un enlace intersante: https://primer.ai/blog/a-new-state-of-the-art-for-named-entity-recognition/\n",
    "\n",
    "* _El nombrar y la necesidad_ (Saul Kripke), 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ner_1 = nlp_es('Jim compró 300 acciones de Acme Corp. en 2006')\n",
    "displacy.render(doc_ner_1, style='ent', jupyter=True, options={'distance':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_ner_2 = nlp_en('Jim bought 300 shares of Acme Corp. in 2006')\n",
    "displacy.render(doc_ner_2, style='ent', jupyter=True, options={'distance':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ner_2 = nlp_en('I heard that Paris Hilton stayed at the Hilton in Paris')\n",
    "displacy.render(doc_ner_2, style='ent', jupyter=True, options={'distance':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}'.format('Token', 'Entity Label'))\n",
    "for entity in doc_ner_1.ents:\n",
    "    print('{0:10}{1:10}'.format(entity.text, entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}'.format('Token', 'Entity Label'))\n",
    "for entity in doc_ner_2.ents:\n",
    "    print('{0:10}{1:10}'.format(entity.text, entity.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Técnica de normalización de textos que busca reducir las palabras a su raíz (lemma).\n",
    "\n",
    "Muy utilizado para reducir la cardinalidad del vocabulario asociando para diferentes formas flexionadas un único token ('entreno', 'entrenarás', 'entrenaría' -> 'entrenar').\n",
    "\n",
    "Aunque muy utilizados en motores de búsqueda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:10}{1:10}{2:10}'.format('Token', 'Lemma', 'PoS Tag'))\n",
    "for idx, token in enumerate(doc):\n",
    "    print('{0:10}{1:10}{2:10}'.format(token.text, token.lemma_, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = 'comer comiendo comieron comedor comeré comerá comerás'\n",
    "text_3 = 'comerás'\n",
    "doc_2 = nlp_es(text_2)\n",
    "doc_3 = nlp_es(text_3)\n",
    "\n",
    "\n",
    "print('Text 2')\n",
    "print('{0:10}{1:10}{2:10}'.format('Token', 'Lemma', 'PoS Tag'))\n",
    "for idx, token in enumerate(doc_2):\n",
    "    print('{0:10}{1:10}{2:10}'.format(token.text, token.lemma_, token.pos_))\n",
    "\n",
    "print('\\nText 3')\n",
    "print('{0:10}{1:10}{2:10}'.format('Token', 'Lemma', 'PoS Tag'))\n",
    "for idx, token in enumerate(doc_3):\n",
    "    print('{0:10}{1:10}{2:10}'.format(token.text, token.lemma_, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization vs Stemming\n",
    "\n",
    "Ambas tienen como objetivo reducir las palabras a su raíz léxica.\n",
    "\n",
    "- **Lemmatization**:\n",
    "\n",
    "    Tiene en consideración el análisis morfológico de las palabras. Son necesarios diccionarios completos de formas flexionadas y raíces (lemmas).\n",
    "    \n",
    "    A veces es necesario desambiguar. P. ej.: \"planta\" (planta vs plantar)\n",
    "\n",
    "<img src=https://blog.bitext.com/hs-fs/hubfs/lemma_v2.png width=500px>\n",
    "\n",
    "- **Stemming**:\n",
    "    \n",
    "    Algoritmos que mediante heurísticos / reglas tratan de reducir las palabras a una posible raíz (stem) mediante la eliminación de algunos prefijos y sufijos. Más sencillo que un lemmatizer\n",
    "    \n",
    "    No hay garantía de que el resultado sea una palabra real.\n",
    "    \n",
    "    El algoritmo más utilizado en Inglés es el de Porter que consiste en 5 fases de reducción de la palabra aplicadas de manera secuencial.\n",
    "    <img src=https://blog.bitext.com/hs-fs/hubfs/stemming_v2.png width=250px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud\n",
    "\n",
    "Si trabajamos con los _small models_ (SM) en lugar de los _large models_ (LM), en lugar de trabajar con _word vectors_ estaremos trabajando con _context-sensitive tensors_.\n",
    "\n",
    "No os preocupéis, los modelos de _word embeddings_ los estudiaremos con mucho más detalle en próximas sesiones :D\n",
    "\n",
    "Como adelanto, en el caso de trabajar con los primeros (word vectors) siempre tendremos el mismo vector para el mismo token. Si trabajamos con los segundos (context-sensitive tensors) el vector del token estará determinado por su contexto (el resto de tokens que le rodean).\n",
    "\n",
    "Aunque el concepto de similitud lo veremos más adelante, como adelanto, conocer que con spaCy es posible calcular lo _parecidos_ o _distintos_ que son dos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.0/es_core_news_md-2.3.0.tar.gz#egg=es_core_news_md==2.3.0 --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_es_md = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_1 = nlp_es_md('verde')\n",
    "word_2 = nlp_es_md('azul')\n",
    "word_3 = nlp_es_md('mariposa')\n",
    "\n",
    "print('Similarity between word {} and word {}: {:0.6f}'.format(1, 2, word_1.similarity(word_2)))\n",
    "print('Similarity between word {} and word {}: {:0.6f}'.format(1, 3, word_1.similarity(word_3)))\n",
    "print('Similarity between word {} and word {}: {:0.6f}'.format(2, 3, word_2.similarity(word_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = nlp_es_md('me gusta el color verde')\n",
    "sent_2 = nlp_es_md('me gusta el azul')\n",
    "sent_3 = nlp_es_md('me gusta la mariposa')\n",
    "\n",
    "print('Similarity between sent {} and sent {}: {:0.6f}'.format(1, 2, sent_1.similarity(sent_2)))\n",
    "print('Similarity between sent {} and sent {}: {:0.6f}'.format(1, 3, sent_1.similarity(sent_3)))\n",
    "print('Similarity between sent {} and sent {}: {:0.6f}'.format(2, 3, sent_2.similarity(sent_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Universo de spaCy\n",
    "\n",
    "Diferentes recursos (paquetes, plugins, extensiones, etc) desarrollados por o para spaCy.\n",
    "\n",
    "https://spacy.io/universe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
