{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "A diferencia de Word2Vec, que trabaja a nivel de palabra, FastText trata de capturar la información morfológica de las palabras.\n",
    "\n",
    ">*\"[...] we propose a new approach **based on the skipgram model, where each word is represented as a bag of character n-grams**. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. [...]\"* <br>(Mikolov et al., Enriching Word Vectors with Subword Information, https://arxiv.org/pdf/1607.04606.pdf)\n",
    "\n",
    "De esta manera, una palabra quedará representada por sus n-grams.\n",
    "\n",
    "El tamaño de los n-grams deberá ser definido como hiperparámetro\n",
    "- min_n: valor mínimo de _n_ a considerar\n",
    "- max_n: valor máximo de _n_ a considerar\n",
    "\n",
    "Ejemplo:\n",
    ">*\"Me gusta el procesado del lenguaje natural\"*\n",
    ">* Ejemplo de *skip-gram* pre-procesado con una ventana de contexto de 2 palabras\n",
    ">\n",
    ">$w_{target} =$ \"procesado\" &emsp;$w_{context} =$ [\"gusta\", \"el\", \"del\", \"lenguaje\"] \n",
    ">\n",
    ">     (\"procesado\", \"gusta\")\n",
    ">\n",
    "> Descomoposición de n-grams con min_n=3 and max_n=4:\n",
    ">\n",
    ">\"procesado\" = [\"$<$pr\", \"pro\", ..., \"ado\", \"do$>$\", \"$<$pro\", \"roce\", ..., \"sado\", \"ado$>$\"]\n",
    ">\n",
    ">* De este modo, la similitud será: <br><br>\n",
    ">&emsp;$\\boxed{s(w_{target}, w_{context}) = \\sum_{g \\in G_{w_{target}}}z_{g}^T v_{w_{context}}}$, where $G_{w_{target}}\\subset\\{g_{1}, ..., g_{G}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras más similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sim_words(word, model1, model2):\n",
    "    query = \"Most similar to {}\".format(word) \n",
    "    print(query)\n",
    "    print(\"-\"*len(query))\n",
    "    for (sim1, sim2) in zip(model1.wv.most_similar(word), model2.wv.most_similar(word)):\n",
    "        print(\"{}:{}{:.3f}{}{}:{}{:.3f}\".format(sim1[0],\n",
    "                                               \" \"*(20-len(sim1[0])), \n",
    "                                               sim1[1], \n",
    "                                               \" \"*10, \n",
    "                                               sim2[0],\n",
    "                                               \" \"*(20-len(sim2[0])),\n",
    "                                               sim2[1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = LineSentence('../../datasets/spanish_news_corpus_doc.txt', limit=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_params = {\n",
    "    # TODO\n",
    "}\n",
    "\n",
    "cbow_params = {\n",
    "    # TODO\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializamos el objeto FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram\n",
    "ft_sg = FastText(**sg_params)\n",
    "\n",
    "# CBOW\n",
    "ft_cbow = FastText(**cbow_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construímos el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram\n",
    "ft_sg.build_vocab(corpus)\n",
    "\n",
    "# CBOW\n",
    "ft_cbow.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulario compuesto por {} palabras'.format(len(ft_sg.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulario compuesto por {} palabras'.format(len(ft_cbow.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos los pesos de los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram\n",
    "ft_sg.train(sentences=corpus, total_examples=ft_sg.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CBOW\n",
    "ft_cbow.train(sentences=corpus, total_examples=ft_cbow.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_sim_words('elecciones', ft_cbow, ft_sg)\n",
    "print_sim_words('botín', ft_cbow, ft_sg)\n",
    "print_sim_words('sánchez', ft_cbow, ft_sg)\n",
    "print_sim_words('rafa', ft_cbow, ft_sg)\n",
    "print_sim_words('impeachment', ft_cbow, ft_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Vocabulary (OOV) Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la cantidad de n-grams creados durante el entrenamiento del FastText hace improbable (que no imposible) que alguna palabra no pueda ser construída como una bolsa de n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'asereje' in ft_sg.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sg.wv.most_similar('asereje')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sg.wv['asereje'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
