{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "A diferencia de Word2Vec, que trabaja a nivel de palabra, FastText trata de capturar la información morfológica de las palabras.\n",
    "\n",
    ">*\"[...] we propose a new approach **based on the skipgram model, where each word is represented as a bag of character n-grams**. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. [...]\"* <br>(Mikolov et al., Enriching Word Vectors with Subword Information, https://arxiv.org/pdf/1607.04606.pdf)\n",
    "\n",
    "De esta manera, una palabra quedará representada por sus n-grams.\n",
    "\n",
    "El tamaño de los n-grams deberá ser definido como hiperparámetro\n",
    "- min_n: valor mínimo de _n_ a considerar\n",
    "- max_n: valor máximo de _n_ a considerar\n",
    "\n",
    "Ejemplo:\n",
    ">*\"Me gusta el procesado del lenguaje natural\"*\n",
    ">* Ejemplo de *skip-gram* pre-procesado con una ventana de contexto de 2 palabras\n",
    ">\n",
    ">$w_{target} =$ \"procesado\" &emsp;$w_{context} =$ [\"gusta\", \"el\", \"del\", \"lenguaje\"] \n",
    ">\n",
    ">     (\"procesado\", \"gusta\")\n",
    ">\n",
    "> Descomoposición de n-grams con min_n=3 and max_n=4:\n",
    ">\n",
    ">\"procesado\" = [\"$<$pr\", \"pro\", ..., \"ado\", \"do$>$\", \"$<$pro\", \"roce\", ..., \"sado\", \"ado$>$\"]\n",
    ">\n",
    ">* De este modo, la similitud será: <br><br>\n",
    ">&emsp;$\\boxed{s(w_{target}, w_{context}) = \\sum_{g \\in G_{w_{target}}}z_{g}^T v_{w_{context}}}$, where $G_{w_{target}}\\subset\\{g_{1}, ..., g_{G}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras más similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sim_words(word, model1, model2):\n",
    "    query = \"Most similar to {}\".format(word) \n",
    "    print(query)\n",
    "    print(\"-\"*len(query))\n",
    "    for (sim1, sim2) in zip(model1.wv.most_similar(word), model2.wv.most_similar(word)):\n",
    "        print(\"{}:{}{:.3f}{}{}:{}{:.3f}\".format(sim1[0],\n",
    "                                               \" \"*(20-len(sim1[0])), \n",
    "                                               sim1[1], \n",
    "                                               \" \"*10, \n",
    "                                               sim2[0],\n",
    "                                               \" \"*(20-len(sim2[0])),\n",
    "                                               sim2[1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = LineSentence('../../datasets/spanish_news_corpus_doc.txt', limit=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_params = {\n",
    "    'sg': 1,\n",
    "    'size': 300,\n",
    "    'min_count': 5,\n",
    "    'window': 5,\n",
    "    'hs': 0,\n",
    "    'negative': 20,\n",
    "    'workers': 4,\n",
    "    'min_n': 3,\n",
    "    'max_n': 6\n",
    "}\n",
    "\n",
    "cbow_params = {\n",
    "    'sg': 0,\n",
    "    'size': 300,\n",
    "    'min_count': 5,\n",
    "    'window': 5,\n",
    "    'hs': 0,\n",
    "    'negative': 20,\n",
    "    'workers': 4,\n",
    "    'min_n': 3,\n",
    "    'max_n': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializamos el objeto FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram\n",
    "ft_sg = FastText(**sg_params)\n",
    "\n",
    "# CBOW\n",
    "ft_cbow = FastText(**cbow_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construímos el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram\n",
    "ft_sg.build_vocab(corpus)\n",
    "\n",
    "# CBOW\n",
    "ft_cbow.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario compuesto por 3139 palabras\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulario compuesto por {} palabras'.format(len(ft_sg.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario compuesto por 3139 palabras\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulario compuesto por {} palabras'.format(len(ft_cbow.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos los pesos de los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram\n",
    "ft_sg.train(sentences=corpus, total_examples=ft_sg.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CBOW\n",
    "ft_cbow.train(sentences=corpus, total_examples=ft_cbow.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sg.save('../../data/ft_sg_d300_mc5_w5.pkl')\n",
    "ft_cbow.save('../../data/ft_cbow_d300_mc5_w5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to elecciones\n",
      "--------------------------\n",
      "funciones:           0.990          funciones:           0.848\n",
      "sanciones:           0.987          posiciones:          0.848\n",
      "condiciones:         0.980          subvenciones:        0.832\n",
      "subvenciones:        0.975          acciones:            0.815\n",
      "acciones:            0.974          reuniones:           0.812\n",
      "vacaciones:          0.971          vacaciones:          0.808\n",
      "negociaciones:       0.971          delegaciones:        0.807\n",
      "situaciones:         0.969          cuestiones:          0.804\n",
      "conversaciones:      0.968          sanciones:           0.799\n",
      "soluciones:          0.968          conclusiones:        0.789\n",
      "\n",
      "\n",
      "Most similar to botín\n",
      "---------------------\n",
      "álvarez:             0.988          álvarez:             0.868\n",
      "archidiócesis:       0.983          elche:               0.850\n",
      "lópez:               0.982          palomera:            0.842\n",
      "igae:                0.978          arena:               0.823\n",
      "champions:           0.978          portavoz:            0.819\n",
      "rfef:                0.977          fernández:           0.819\n",
      "fernández:           0.975          toledo:              0.817\n",
      "pérez:               0.974          ortega:              0.816\n",
      "vraem:               0.974          ángel:               0.816\n",
      "iberdrola:           0.972          albert:              0.814\n",
      "\n",
      "\n",
      "Most similar to sánchez\n",
      "-----------------------\n",
      "trump:               0.929          pedro:               0.848\n",
      "cs:                  0.925          cs:                  0.804\n",
      "pedro:               0.922          investidura:         0.796\n",
      "considera:           0.917          govern:              0.775\n",
      "afirma:              0.910          naranja:             0.769\n",
      "señala:              0.909          rivera:              0.766\n",
      "psoe:                0.904          155:                 0.757\n",
      "govern:              0.897          cospedal:            0.745\n",
      "puig:                0.895          ejecutar:            0.737\n",
      "gobierno:            0.895          pedir:               0.733\n",
      "\n",
      "\n",
      "Most similar to rafa\n",
      "--------------------\n",
      "fifa:                0.992          sibuya:              0.819\n",
      "cerró:               0.985          raíz:                0.813\n",
      "domicilio:           0.984          fifa:                0.808\n",
      "norteamericano:      0.981          norteamericano:      0.803\n",
      "berlín:              0.979          ratio:               0.797\n",
      "ranking:             0.978          apertura:            0.791\n",
      "término:             0.978          tarifa:              0.791\n",
      "ubago:               0.977          jaén:                0.788\n",
      "refleja:             0.977          temporada:           0.787\n",
      "08:                  0.974          rusia:               0.786\n",
      "\n",
      "\n",
      "Most similar to impeachment\n",
      "---------------------------\n",
      "concretamente:       0.978          pekín:               0.841\n",
      "coche:               0.970          donald:              0.820\n",
      "automáticamente:     0.963          lautenschläger:      0.811\n",
      "impulso:             0.958          trump:               0.808\n",
      "pekín:               0.955          abierto:             0.762\n",
      "código:              0.953          ataque:              0.755\n",
      "impedir:             0.952          puigdemont:          0.754\n",
      "orden:               0.952          parlament:           0.753\n",
      "paquete:             0.952          impedir:             0.750\n",
      "ruso:                0.951          optimismo:           0.745\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_sim_words('elecciones', ft_cbow, ft_sg)\n",
    "print_sim_words('botín', ft_cbow, ft_sg)\n",
    "print_sim_words('sánchez', ft_cbow, ft_sg)\n",
    "print_sim_words('rafa', ft_cbow, ft_sg)\n",
    "print_sim_words('impeachment', ft_cbow, ft_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Vocabulary (OOV) Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la cantidad de n-grams creados durante el entrenamiento del FastText hace improbable (que no imposible) que alguna palabra no pueda ser construída como una bolsa de n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asereje' in ft_sg.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vilamitjana', 0.8189496994018555),\n",
       " ('blog', 0.8152294754981995),\n",
       " ('adquirir', 0.8094627857208252),\n",
       " ('bitcoin', 0.8001219034194946),\n",
       " ('antigua', 0.7970510721206665),\n",
       " ('envió', 0.7890565395355225),\n",
       " ('streaming', 0.7890428304672241),\n",
       " ('enorme', 0.7873703241348267),\n",
       " ('discurso', 0.7858148813247681),\n",
       " ('moverse', 0.78006511926651)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_sg.wv.most_similar('asereje')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_sg.wv['asereje'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
