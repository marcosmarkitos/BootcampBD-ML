{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2a Clasificación imágenes.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ok2mQSW_4t-uCPzDerLjJjl6N6Ly2QFp","authorship_tag":"ABX9TyMZXg1iix+H3SlDQKbR4lvK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CF3PrvWJ6X8t","colab_type":"text"},"source":["**CLASIFICACIÓN PARTIENDO DE IMÁGENES**\n","\n","En este notebook vamos a tratar de clasificar los apartamentos según su precio en baratos, medios o caros. Para ello estableceremos los límites en 50€ para los baratos y 150€ para los caros.\n","Dicha predicción se va a hacer a partir de las imágenes que tenemos en el dataset de airbnb que venimos usando en las prácticas de este Bootcamp.\n","\n","En primer lugar nos descargamos el fichero de internet y lo copiamos en un directorio local de My Drive donde tenemos recogido todo el entorno de esta práctica.\n","A continuación hacemos lo mismo con las imágenes de cada una de las entradas. Usamos la vista en miniatura que sacamos de la URL de dicho fichero.\n","\n","También montamos el google collab con My Drive para tenerlo vinculado.\n","\n","Estos pasos solo hay que realizarlos la primera vez, una vez que tenemos los ficheros en My Drive se pueden saltar y pasamos a cargar los datos directamente desde dicho directorio."]},{"cell_type":"code","metadata":{"id":"yGAWhUGP6nRG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1593064923489,"user_tz":-120,"elapsed":62620,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"a49fee41-1611-48e3-9bd8-5cb4329ec326"},"source":["# nos descargamos el dataset de OpenDataSoft\n","!wget -O \"airbnb-listings.csv\" \"https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\"\n","\n","!ls -lah"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-25 06:01:03--  https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\n","Resolving public.opendatasoft.com (public.opendatasoft.com)... 34.249.199.226, 34.248.20.69\n","Connecting to public.opendatasoft.com (public.opendatasoft.com)|34.249.199.226|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/csv]\n","Saving to: ‘airbnb-listings.csv’\n","\n","airbnb-listings.csv     [  <=>               ]  54.19M  2.77MB/s    in 50s     \n","\n","2020-06-25 06:02:01 (1.09 MB/s) - ‘airbnb-listings.csv’ saved [56826824]\n","\n","total 55M\n","drwxr-xr-x 1 root root 4.0K Jun 25 06:01 .\n","drwxr-xr-x 1 root root 4.0K Jun 25 05:59 ..\n","-rw-r--r-- 1 root root  55M Jun 25 06:02 airbnb-listings.csv\n","drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n","drwx------ 4 root root 4.0K Jun 25 06:00 drive\n","drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-4dDxIX4wNAb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1593148776645,"user_tz":-120,"elapsed":8497,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"d123b864-a9b6-4071-b32b-17b8a663be88"},"source":["!ls -lah"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 16K\n","drwxr-xr-x 1 root root 4.0K Jun 17 16:18 .\n","drwxr-xr-x 1 root root 4.0K Jun 26 05:15 ..\n","drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n","drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YLSxaq6U32t_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27R_6DOoEpEF","colab_type":"code","colab":{}},"source":["!cp airbnb-listings.csv \"drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyUGFt37FHVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1593149243426,"user_tz":-120,"elapsed":875,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"49df4fd7-e4e7-4e33-e62f-2b9c7e4ff32f"},"source":["# aquí creamos nuestra estructura de datos, que va a consistir en la url de la\n","# imagen y un índice para saber donde insertarla en nuestro array\n","images_paths = [[i, img_url] for i, img_url in enumerate(full_df['Thumbnail Url'])]\n","images_paths[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0,\n","  'https://a0.muscache.com/im/pictures/cffe393a-0d84-4fd5-ab4c-a62e067c1b0d.jpg?aki_policy=small'],\n"," [1,\n","  'https://a0.muscache.com/im/pictures/ea919e56-aa99-4d5d-a129-1edf0d117d6a.jpg?aki_policy=small'],\n"," [2,\n","  'https://a0.muscache.com/im/pictures/57011236/eea5c213_original.jpg?aki_policy=small'],\n"," [3,\n","  'https://a0.muscache.com/im/pictures/974f0245-55c2-4e8c-b9bf-14c1c975c798.jpg?aki_policy=small'],\n"," [4,\n","  'https://a0.muscache.com/im/pictures/c2dde263-20dd-43af-8c6b-be636c2c0ce1.jpg?aki_policy=small']]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"My93mt1qMtcv","colab_type":"code","colab":{}},"source":["import imageio as io\n","import cv2\n","\n","# esta es la función que se descargará la imagen y devolverá la imagen y el \n","# índice indicando la posición donde se incrustará la imagen en nuestro array\n","def get_image(data_url, target_size=(224, 224)):\n","    idx, url = data_url\n","    try:\n","        img = io.imread(url)\n","        # hay alguna imagen en blanco y negro y daría error al incluirla en \n","        # nuestro array de imagenes que tiene 3 canales, así que convertimos\n","        # todas las imágenes que tengan menos de 3 dimensiones a color\n","        if img.ndim < 3:\n","            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","        img = cv2.resize(img, dsize=target_size)\n","        return img, idx\n","    except IOError as err:\n","        return (None, idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WldeZWd2NAbm","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# en este array iremos incrustando las imágenes conforme las vayamos obteniendo\n","loaded_images = np.zeros((len(images_paths), 224, 224, 3), dtype=np.uint8)\n","\n","# y en este array llevaremos un control de cuales se han cargado correctamente\n","# y cuales no\n","was_loaded = np.zeros(len(images_paths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4PSNB_hNVLC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593151722336,"user_tz":-120,"elapsed":498858,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"e1f758ce-a160-4e1e-8b4b-0e7fdf5ac77e"},"source":["import concurrent\n","from tqdm import tqdm\n","\n","# creamos un pool de procesos que se irán descargando las imágenes\n","# por defecto, se crearán tantos como CPUs tenga vuestra máquina\n","with concurrent.futures.ProcessPoolExecutor() as executor:\n","    # procesamos la lista de urls de imágenes paralelizandola con el pool de procesos\n","    for (img, idx) in tqdm(executor.map(get_image, images_paths), total=len(images_paths)):\n","        # metemos la imagen en nuestro array\n","        if img is not None:\n","            loaded_images[idx] = img\n","            was_loaded[idx] = 1\n","        else:\n","            was_loaded[idx] = 0\n","\n","print('Terminado!')\n","print(f'Total de imágenes recuperadas correctamente: {sum(was_loaded)}/{len(images_paths)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 14001/14001 [08:14<00:00, 28.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Terminado!\n","Total de imágenes recuperadas correctamente: 11271.0/14001\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"INHNB-WhNsvG","colab_type":"code","colab":{}},"source":["# guardamos las imágenes (y yo os recomiendo que os lo guardéis en GDrive para evitar tener que repetir esto)\n","np.save('images.npy', loaded_images)\n","np.save('was_loaded.npy', was_loaded)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4-TANTHNsWk","colab_type":"code","colab":{}},"source":["# almacenamos las imagenes en nuestro drive\n","!cp images.npy \"drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning\"\n","!cp was_loaded.npy \"drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJu_2Fcm8qNz","colab_type":"text"},"source":["Esta parte de descarga, montado y copiado solo hace falta ejecutarla la primera vez. Una vez que lo tenemos copiado solo necesitamos cargarlo directamente.\n","\n","A partir de aquí empieza nuestro ejercicio de clasificación.\n","\n","Como hábito de buena costumbre, para no incurrir en errores involuntarios, en primer lugar se va a dividir el dataset original en train, validation y test.\n","\n","Se trabaja únicamente con el de train con el objetivo de elegir un modelo. Eso se verifica con el conjunto de validation y finalmente se aplica ese \"entrenamiento\" al bloque de test."]},{"cell_type":"code","metadata":{"id":"E63PIJHrVKuF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593236907095,"user_tz":-120,"elapsed":724,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"eb869658-6b81-4d92-d0a5-b00f6bc2037e"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GQK8_P3qVKOb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1593236918896,"user_tz":-120,"elapsed":11873,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"c4e0e25a-8b97-4c0e-da92-48f186f3c52f"},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","%matplotlib inline\n","cm = plt.cm.RdBu\n","cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","#hacemos la divisón en train, val y test\n","full_df = pd.read_csv('drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning/airbnb-listings.csv', sep=';', decimal='.')\n","full_train, test = train_test_split(full_df, test_size=0.2, shuffle=True, random_state=0)\n","train, val = train_test_split(full_train, test_size=0.2, shuffle=True, random_state=0)\n","\n","print(f'Dimensiones del dataset de training: {train.shape}')\n","print(f'Dimensiones del dataset de validación: {val.shape}')\n","print(f'Dimensiones del dataset de test: {test.shape}')\n","\n","# Guardamos\n","train.to_csv('./train.csv', sep=';', decimal='.', index=True)\n","val.to_csv('./val.csv', sep=';', decimal='.', index=True)\n","test.to_csv('./test.csv', sep=';', decimal='.', index=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (8960, 89)\n","Dimensiones del dataset de validación: (2240, 89)\n","Dimensiones del dataset de test: (2801, 89)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"snh4Brs5SLwN","colab_type":"code","colab":{}},"source":["#cargamos las imágenes desde el directorio de My Drive (ya las habíamos descargado previamente)\n","images  = np.load('drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning/images.npy')\n","was_loaded  = np.load('drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning/was_loaded.npy')\n","\n","#cargamos los datos ya divididos en train, val y test\n","df_train = pd.read_csv('./train.csv', sep=';', decimal='.')\n","df_val = pd.read_csv('./val.csv', sep=';', decimal='.')\n","df_test = pd.read_csv('./test.csv', sep=';', decimal='.')\n","\n","#usando el índice de la división anterior obtenemos los conjuntos de test, val y test en las imágenes\n","train_imgs = images[df_train['Unnamed: 0']]\n","val_imgs = images[df_val['Unnamed: 0']]\n","test_imgs = images[df_test['Unnamed: 0']]\n","\n","train_was_loaded = was_loaded[df_train['Unnamed: 0']]\n","val_was_loaded = was_loaded[df_val['Unnamed: 0']]\n","test_was_loaded = was_loaded[df_test['Unnamed: 0']]\n","\n","print(f'Dimensiones del dataset de training: {train_imgs.shape}')\n","print(f'Dimensiones del dataset de validación: {val_imgs.shape}')\n","print(f'Dimensiones del dataset de test: {test_imgs.shape}')\n","\n","print(f'Dimensiones del dataset de training: {train_was_loaded.shape}')\n","print(f'Dimensiones del dataset de validación: {val_was_loaded.shape}')\n","print(f'Dimensiones del dataset de test: {test_was_loaded.shape}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oveWlY0PQu8G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1593236979934,"user_tz":-120,"elapsed":69294,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"1e4a00c3-9f98-4e4e-fcad-c58c19f21e76"},"source":["# nos quedamos con los datos e imágenes para los que hemos podido encontrar imágenes\n","train_imgs_loaded = train_imgs[train_was_loaded == 1]\n","val_imgs_loaded = val_imgs[val_was_loaded == 1]\n","test_imgs_loaded = test_imgs[test_was_loaded == 1]\n","\n","train_with_imgs = df_train[train_was_loaded == 1]\n","val_with_imgs = df_val[val_was_loaded == 1]\n","test_with_imgs = df_test[test_was_loaded == 1]\n","\n","print(f'Dimensiones del dataset de training: {train_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de validación: {val_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de test: {test_imgs_loaded.shape}')\n","\n","print(f'Dimensiones del dataset de training: {train_with_imgs.shape}')\n","print(f'Dimensiones del dataset de validación: {val_with_imgs.shape}')\n","print(f'Dimensiones del dataset de test: {test_with_imgs.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (7204, 224, 224, 3)\n","Dimensiones del dataset de validación: (1790, 224, 224, 3)\n","Dimensiones del dataset de test: (2277, 224, 224, 3)\n","Dimensiones del dataset de training: (7204, 90)\n","Dimensiones del dataset de validación: (1790, 90)\n","Dimensiones del dataset de test: (2277, 90)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fWLeux1ndb7l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1593236979937,"user_tz":-120,"elapsed":61804,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"84636b8d-e543-475a-c86d-c2219f874263"},"source":["#PRICE\n","#imputamos valores vacíos con la media de train\n","MeanPriceTrain = train_with_imgs['Price'].mean()\n","train_with_imgs['Price'].fillna(MeanPriceTrain, inplace=True)\n","val_with_imgs['Price'].fillna(MeanPriceTrain, inplace=True)\n","test_with_imgs['Price'].fillna(MeanPriceTrain, inplace=True)\n","#definimos outlier >400€\n","#indexTrainFiltered = train_with_imgs[train_with_imgs['Price']>400].index\n","#train_with_imgs.drop(indexTrainFiltered, inplace=True)\n","#indexValFiltered = val_with_imgs[val_with_imgs['Price']>400].index\n","#val_with_imgs.drop(indexValFiltered, inplace=True)\n","#indexTestFiltered = test_with_imgs[test_with_imgs['Price']>400].index\n","#test_with_imgs.drop(indexTestFiltered, inplace=True)\n","\n","#train_imgs_loaded.drop(indexTrainFiltered, inplace=True)\n","#val_imgs_loaded.drop(indexValFiltered, inplace=True)\n","#test_imgs_loaded.drop(indexTestFiltered, inplace=True)\n","\n","#transformamos variable Price a gausiana\n","train_with_imgs['Price'] = train_with_imgs['Price'].apply(lambda x: np.log10(x))\n","val_with_imgs['Price'] = val_with_imgs['Price'].apply(lambda x: np.log10(x))\n","test_with_imgs['Price'] = test_with_imgs['Price'].apply(lambda x: np.log10(x))\n","#categorizamos la variable precio en 3 tipos: barato (0), medio (1) y caro (2).\n","train_with_imgs['Cat_Price'] = train_with_imgs['Price'].apply(lambda x: 0 if x < np.log10(50) else (1 if x < np.log10(150) else 2))\n","val_with_imgs['Cat_Price'] = val_with_imgs['Price'].apply(lambda x: 0 if x < np.log10(50) else (1 if x < np.log10(150) else 2))\n","test_with_imgs['Cat_Price'] = test_with_imgs['Price'].apply(lambda x: 0 if x < np.log10(50) else (1 if x < np.log10(150) else 2))\n","\n","print(f'Dimensiones del dataset de training: {train_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de validación: {val_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de test: {test_imgs_loaded.shape}')\n","\n","print(f'Dimensiones del dataset de training: {train_with_imgs.shape}')\n","print(f'Dimensiones del dataset de validación: {val_with_imgs.shape}')\n","print(f'Dimensiones del dataset de test: {test_with_imgs.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (7204, 224, 224, 3)\n","Dimensiones del dataset de validación: (1790, 224, 224, 3)\n","Dimensiones del dataset de test: (2277, 224, 224, 3)\n","Dimensiones del dataset de training: (7204, 91)\n","Dimensiones del dataset de validación: (1790, 91)\n","Dimensiones del dataset de test: (2277, 91)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5CGH2N4oH0vy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1593236989110,"user_tz":-120,"elapsed":1538,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"3b7187b0-37d6-4eac-f978-74c7f386e633"},"source":["#Redimensionamos las imágenes de entrada. Estoy teniendo problemas de RAM y no puedo ejecutarlo\n","#con 224x224 no puedo escalar /255. Con 112x112 no puedo ejecutar el modelo\n","#es necesario asumir esta pérdida de información\n","train_imgs_loaded = np.resize(train_imgs_loaded, (train_imgs_loaded.shape[0],64, 64, train_imgs_loaded.shape[3]))\n","val_imgs_loaded = np.resize(val_imgs_loaded, (val_imgs_loaded.shape[0],64, 64, val_imgs_loaded.shape[3]))\n","test_imgs_loaded = np.resize(test_imgs_loaded, (test_imgs_loaded.shape[0],64, 64, test_imgs_loaded.shape[3]))\n","\n","print(f'Dimensiones del dataset de training: {train_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de training: {val_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de training: {test_imgs_loaded.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (7204, 64, 64, 3)\n","Dimensiones del dataset de training: (1790, 64, 64, 3)\n","Dimensiones del dataset de training: (2277, 64, 64, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J7QHeDgeJzbC","colab_type":"code","colab":{}},"source":["#escalamos los datos de entrada. Lo hago en celdas separadas ya que hay algún problema de RAM\n","#se trata de imágenes así que no hace falta centrar, solo dividimos por el máximo. \n","# nos aseguramos de hacerlo como float para no perder la info de los decimales\n","\n","train_imgs_loaded = train_imgs_loaded.astype('float32') / 255.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lgn_PkHKiiV","colab_type":"code","colab":{}},"source":["val_imgs_loaded = val_imgs_loaded.astype('float32') / 255.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"La20DOHfLjYl","colab_type":"code","colab":{}},"source":["test_imgs_loaded = test_imgs_loaded.astype('float32') / 255."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvOWPiKEMIi5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593236996003,"user_tz":-120,"elapsed":517,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"aa44c48e-67b7-4d68-b2a2-664e1c1d47de"},"source":["from keras.utils import to_categorical\n","\n","# convertimos las etiquetas a one-hot encoding\n","num_classes = 3\n","Ytrain = to_categorical(train_with_imgs['Cat_Price'], num_classes)\n","Yval = to_categorical(val_with_imgs['Cat_Price'], num_classes)\n","Ytest = to_categorical(test_with_imgs['Cat_Price'], num_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"-ZSSts1O9-zF","colab_type":"text"},"source":["Una vez que ya tenemos los datos de las imágenes y la variable objetivo preparados, definimos los modelos con los que vamos a ir trabajando.\n","\n","La idea general es usar una red neuronal convolucional (CNN) donde las primeras capas actúan como extractor de características y añadimos un clasificador final."]},{"cell_type":"code","metadata":{"id":"wvbGjc6p9q9n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"ok","timestamp":1593238393012,"user_tz":-120,"elapsed":99942,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"4800422b-0fd9-461c-f4f6-26f261d968b7"},"source":["# imports necesarios\n","import numpy as np\n","from keras.datasets import cifar10\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Flatten\n","from keras.layers.convolutional import Conv2D\n","from keras.optimizers import Adam\n","from keras.layers.pooling import MaxPooling2D\n","from keras.layers import BatchNormalization, Activation\n","from keras.layers import Dropout\n","from keras.utils import to_categorical\n","\n","# Inizializamos el modelo\n","model = Sequential()\n","\n","# Definimos una capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), input_shape=(64, 64, 3)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una segunda capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una tercera capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Añadimos nuestro clasificador\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compilamos el modelo\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=0.0001, decay=1e-6),\n","              metrics=['accuracy'])\n","\n","# Entrenamos el modelo\n","model.fit(train_imgs_loaded, Ytrain,\n","          batch_size=128,\n","          shuffle=True,\n","          epochs=10,\n","          validation_data=(val_imgs_loaded, Yval)) \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 7204 samples, validate on 1790 samples\n","Epoch 1/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 1.6351 - accuracy: 0.4554 - val_loss: 1.0973 - val_accuracy: 0.4480\n","Epoch 2/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 1.0859 - accuracy: 0.4622 - val_loss: 1.0961 - val_accuracy: 0.4480\n","Epoch 3/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.9507 - accuracy: 0.4729 - val_loss: 1.0945 - val_accuracy: 0.4480\n","Epoch 4/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.9193 - accuracy: 0.4804 - val_loss: 1.0931 - val_accuracy: 0.4480\n","Epoch 5/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.9129 - accuracy: 0.4738 - val_loss: 1.0919 - val_accuracy: 0.4480\n","Epoch 6/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.9045 - accuracy: 0.4736 - val_loss: 1.0894 - val_accuracy: 0.4480\n","Epoch 7/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.9010 - accuracy: 0.4847 - val_loss: 1.0873 - val_accuracy: 0.4480\n","Epoch 8/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.9005 - accuracy: 0.4702 - val_loss: 1.0744 - val_accuracy: 0.4704\n","Epoch 9/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.8968 - accuracy: 0.4797 - val_loss: 1.0537 - val_accuracy: 0.4765\n","Epoch 10/10\n","7204/7204 [==============================] - 10s 1ms/step - loss: 0.8940 - accuracy: 0.4692 - val_loss: 1.0011 - val_accuracy: 0.4844\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fcbbf83cf98>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"SMOVGd_3_bNl","colab_type":"text"},"source":["En los tres bloques convolucionales que estamos definiendo se observa la misma estructura: una capa convolucional, una capa de batchNormalization, otra de MaxPooling y otra de Dropout. Estas tres últimas tienen como objetivo diezmar el contenido de la salida de la capa convolucional. Esto se hace para quitar complejidad y reducir el overfitting del modelo. En los siguientes apartados iremos probando con distintos valores y configuraciones a ver cuál es el mejor resultado que podemos obtener."]},{"cell_type":"code","metadata":{"id":"zKE1A-tdWIOl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"ok","timestamp":1593237429295,"user_tz":-120,"elapsed":68642,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"624fe99e-dfe9-4bf2-ed35-0855b104be2c"},"source":["#quito el BatchNormalization\n","\n","# Inizializamos el modelo\n","model = Sequential()\n","\n","# Definimos una capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), input_shape=(64, 64, 3)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una segunda capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una tercera capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Añadimos nuestro clasificador\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compilamos el modelo\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=0.0001, decay=1e-6),\n","              metrics=['accuracy'])\n","\n","# Entrenamos el modelo\n","model.fit(train_imgs_loaded, Ytrain,\n","          batch_size=128,\n","          shuffle=True,\n","          epochs=10,\n","          validation_data=(val_imgs_loaded, Yval)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 7204 samples, validate on 1790 samples\n","Epoch 1/10\n","7204/7204 [==============================] - 7s 1ms/step - loss: 0.9146 - accuracy: 0.4618 - val_loss: 0.9430 - val_accuracy: 0.4955\n","Epoch 2/10\n","7204/7204 [==============================] - 7s 907us/step - loss: 0.8954 - accuracy: 0.4713 - val_loss: 0.9592 - val_accuracy: 0.4944\n","Epoch 3/10\n","7204/7204 [==============================] - 7s 919us/step - loss: 0.8975 - accuracy: 0.4636 - val_loss: 0.9637 - val_accuracy: 0.4978\n","Epoch 4/10\n","7204/7204 [==============================] - 7s 910us/step - loss: 0.8942 - accuracy: 0.4672 - val_loss: 0.9543 - val_accuracy: 0.4721\n","Epoch 5/10\n","7204/7204 [==============================] - 7s 911us/step - loss: 0.8922 - accuracy: 0.4771 - val_loss: 0.9727 - val_accuracy: 0.4520\n","Epoch 6/10\n","7204/7204 [==============================] - 7s 916us/step - loss: 0.8928 - accuracy: 0.4772 - val_loss: 0.9597 - val_accuracy: 0.4950\n","Epoch 7/10\n","7204/7204 [==============================] - 7s 913us/step - loss: 0.8918 - accuracy: 0.4671 - val_loss: 0.9528 - val_accuracy: 0.4939\n","Epoch 8/10\n","7204/7204 [==============================] - 7s 915us/step - loss: 0.8922 - accuracy: 0.4638 - val_loss: 0.9489 - val_accuracy: 0.4866\n","Epoch 9/10\n","7204/7204 [==============================] - 7s 918us/step - loss: 0.8896 - accuracy: 0.4729 - val_loss: 0.9342 - val_accuracy: 0.4704\n","Epoch 10/10\n","7204/7204 [==============================] - 7s 918us/step - loss: 0.8892 - accuracy: 0.4792 - val_loss: 0.9193 - val_accuracy: 0.4793\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fcbc5f86f28>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"KQIS4STWWIFD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"ok","timestamp":1593237615544,"user_tz":-120,"elapsed":69628,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"9ca92dcc-2bfa-465c-9c69-0f72ed478565"},"source":["#ahora añadimos MaxNormalization en el clasificador\n","\n","from keras.constraints import max_norm\n","\n","# Inizializamos el modelo\n","model = Sequential()\n","\n","# Definimos una capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), input_shape=(64, 64, 3)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una segunda capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una tercera capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Añadimos nuestro clasificador\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu', kernel_constraint=max_norm(3.)))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compilamos el modelo\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=0.0001, decay=1e-6),\n","              metrics=['accuracy'])\n","\n","# Entrenamos el modelo\n","model.fit(train_imgs_loaded, Ytrain,\n","          batch_size=128,\n","          shuffle=True,\n","          epochs=10,\n","          validation_data=(val_imgs_loaded, Yval)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 7204 samples, validate on 1790 samples\n","Epoch 1/10\n","7204/7204 [==============================] - 7s 989us/step - loss: 0.9146 - accuracy: 0.4771 - val_loss: 0.9747 - val_accuracy: 0.4950\n","Epoch 2/10\n","7204/7204 [==============================] - 7s 927us/step - loss: 0.8988 - accuracy: 0.4678 - val_loss: 0.9836 - val_accuracy: 0.4827\n","Epoch 3/10\n","7204/7204 [==============================] - 7s 931us/step - loss: 0.8973 - accuracy: 0.4653 - val_loss: 0.9835 - val_accuracy: 0.4665\n","Epoch 4/10\n","7204/7204 [==============================] - 7s 921us/step - loss: 0.8952 - accuracy: 0.4678 - val_loss: 0.9762 - val_accuracy: 0.4464\n","Epoch 5/10\n","7204/7204 [==============================] - 7s 927us/step - loss: 0.8929 - accuracy: 0.4646 - val_loss: 0.9766 - val_accuracy: 0.4916\n","Epoch 6/10\n","7204/7204 [==============================] - 7s 931us/step - loss: 0.8949 - accuracy: 0.4557 - val_loss: 0.9860 - val_accuracy: 0.4944\n","Epoch 7/10\n","7204/7204 [==============================] - 7s 933us/step - loss: 0.8907 - accuracy: 0.4771 - val_loss: 0.9498 - val_accuracy: 0.5034\n","Epoch 8/10\n","7204/7204 [==============================] - 7s 931us/step - loss: 0.8906 - accuracy: 0.4690 - val_loss: 0.9189 - val_accuracy: 0.4950\n","Epoch 9/10\n","7204/7204 [==============================] - 7s 932us/step - loss: 0.8908 - accuracy: 0.4703 - val_loss: 0.9607 - val_accuracy: 0.4978\n","Epoch 10/10\n","7204/7204 [==============================] - 7s 932us/step - loss: 0.8927 - accuracy: 0.4700 - val_loss: 0.9548 - val_accuracy: 0.4810\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fcbc1013ac8>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"lm0wWxtbAa3-","colab_type":"text"},"source":["Viendo los tres resultados anteriores son muy similares, pero parece que este último es algo mejor 49% de accuracy frente a 47% y 48%).\n","\n","Por esta razón seguimos con este modelo y aumentamos el número de épocas."]},{"cell_type":"code","metadata":{"id":"RLwnYqQIWH61","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593238140985,"user_tz":-120,"elapsed":334886,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"f8ae8ef8-d834-4217-bb3f-b3a8145f980c"},"source":["# Inizializamos el modelo\n","model = Sequential()\n","\n","# Definimos una capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), input_shape=(64, 64, 3)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una segunda capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Definimos una tercera capa convolucional\n","model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Añadimos nuestro clasificador\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu', kernel_constraint=max_norm(3.)))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compilamos el modelo\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=0.0001, decay=1e-6),\n","              metrics=['accuracy'])\n","\n","# Entrenamos el modelo\n","model.fit(train_imgs_loaded, Ytrain,\n","          batch_size=128,\n","          shuffle=True,\n","          epochs=50,\n","          validation_data=(val_imgs_loaded, Yval)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 7204 samples, validate on 1790 samples\n","Epoch 1/50\n","7204/7204 [==============================] - 7s 991us/step - loss: 0.9112 - accuracy: 0.4732 - val_loss: 0.9528 - val_accuracy: 0.4480\n","Epoch 2/50\n","7204/7204 [==============================] - 7s 925us/step - loss: 0.9006 - accuracy: 0.4679 - val_loss: 0.9851 - val_accuracy: 0.4989\n","Epoch 3/50\n","7204/7204 [==============================] - 7s 922us/step - loss: 0.8959 - accuracy: 0.4704 - val_loss: 0.9623 - val_accuracy: 0.4950\n","Epoch 4/50\n","7204/7204 [==============================] - 7s 918us/step - loss: 0.8945 - accuracy: 0.4697 - val_loss: 0.9660 - val_accuracy: 0.4955\n","Epoch 5/50\n","7204/7204 [==============================] - 7s 925us/step - loss: 0.8914 - accuracy: 0.4808 - val_loss: 0.9639 - val_accuracy: 0.4939\n","Epoch 6/50\n","7204/7204 [==============================] - 7s 922us/step - loss: 0.8928 - accuracy: 0.4695 - val_loss: 0.9596 - val_accuracy: 0.4978\n","Epoch 7/50\n","7204/7204 [==============================] - 7s 928us/step - loss: 0.8902 - accuracy: 0.4702 - val_loss: 0.9535 - val_accuracy: 0.4570\n","Epoch 8/50\n","7204/7204 [==============================] - 7s 930us/step - loss: 0.8906 - accuracy: 0.4685 - val_loss: 0.9607 - val_accuracy: 0.4475\n","Epoch 9/50\n","7204/7204 [==============================] - 7s 922us/step - loss: 0.8910 - accuracy: 0.4749 - val_loss: 0.9572 - val_accuracy: 0.4877\n","Epoch 10/50\n","7204/7204 [==============================] - 7s 926us/step - loss: 0.8892 - accuracy: 0.4742 - val_loss: 0.9405 - val_accuracy: 0.4955\n","Epoch 11/50\n","7204/7204 [==============================] - 7s 926us/step - loss: 0.8881 - accuracy: 0.4796 - val_loss: 0.9245 - val_accuracy: 0.4872\n","Epoch 12/50\n","7204/7204 [==============================] - 7s 921us/step - loss: 0.8885 - accuracy: 0.4747 - val_loss: 0.9441 - val_accuracy: 0.4883\n","Epoch 13/50\n","7204/7204 [==============================] - 7s 922us/step - loss: 0.8879 - accuracy: 0.4754 - val_loss: 0.9267 - val_accuracy: 0.4575\n","Epoch 14/50\n","7204/7204 [==============================] - 7s 925us/step - loss: 0.8877 - accuracy: 0.4743 - val_loss: 0.9185 - val_accuracy: 0.4464\n","Epoch 15/50\n","7204/7204 [==============================] - 7s 920us/step - loss: 0.8874 - accuracy: 0.4660 - val_loss: 0.9074 - val_accuracy: 0.4536\n","Epoch 16/50\n","7204/7204 [==============================] - 7s 928us/step - loss: 0.8865 - accuracy: 0.4721 - val_loss: 0.9087 - val_accuracy: 0.4950\n","Epoch 17/50\n","7204/7204 [==============================] - 7s 924us/step - loss: 0.8852 - accuracy: 0.4742 - val_loss: 0.9019 - val_accuracy: 0.4950\n","Epoch 18/50\n","7204/7204 [==============================] - 7s 922us/step - loss: 0.8867 - accuracy: 0.4822 - val_loss: 0.9013 - val_accuracy: 0.4676\n","Epoch 19/50\n","7204/7204 [==============================] - 7s 927us/step - loss: 0.8862 - accuracy: 0.4700 - val_loss: 0.8984 - val_accuracy: 0.4821\n","Epoch 20/50\n","7204/7204 [==============================] - 7s 927us/step - loss: 0.8861 - accuracy: 0.4767 - val_loss: 0.8950 - val_accuracy: 0.4966\n","Epoch 21/50\n","7204/7204 [==============================] - 7s 926us/step - loss: 0.8851 - accuracy: 0.4758 - val_loss: 0.8900 - val_accuracy: 0.4598\n","Epoch 22/50\n","7204/7204 [==============================] - 7s 925us/step - loss: 0.8844 - accuracy: 0.4684 - val_loss: 0.8870 - val_accuracy: 0.4564\n","Epoch 23/50\n","7204/7204 [==============================] - 7s 923us/step - loss: 0.8860 - accuracy: 0.4674 - val_loss: 0.8896 - val_accuracy: 0.4911\n","Epoch 24/50\n","7204/7204 [==============================] - 7s 919us/step - loss: 0.8855 - accuracy: 0.4735 - val_loss: 0.8856 - val_accuracy: 0.4911\n","Epoch 25/50\n","7204/7204 [==============================] - 7s 924us/step - loss: 0.8848 - accuracy: 0.4661 - val_loss: 0.8854 - val_accuracy: 0.4458\n","Epoch 26/50\n","7204/7204 [==============================] - 7s 925us/step - loss: 0.8829 - accuracy: 0.4829 - val_loss: 0.8866 - val_accuracy: 0.4972\n","Epoch 27/50\n","7204/7204 [==============================] - 7s 923us/step - loss: 0.8823 - accuracy: 0.4789 - val_loss: 0.8823 - val_accuracy: 0.4816\n","Epoch 28/50\n","7204/7204 [==============================] - 7s 929us/step - loss: 0.8845 - accuracy: 0.4758 - val_loss: 0.8845 - val_accuracy: 0.4888\n","Epoch 29/50\n","7204/7204 [==============================] - 7s 926us/step - loss: 0.8819 - accuracy: 0.4824 - val_loss: 0.8813 - val_accuracy: 0.4972\n","Epoch 30/50\n","7204/7204 [==============================] - 7s 926us/step - loss: 0.8816 - accuracy: 0.4897 - val_loss: 0.8779 - val_accuracy: 0.4603\n","Epoch 31/50\n","7204/7204 [==============================] - 7s 916us/step - loss: 0.8837 - accuracy: 0.4739 - val_loss: 0.8823 - val_accuracy: 0.4531\n","Epoch 32/50\n","7204/7204 [==============================] - 7s 911us/step - loss: 0.8816 - accuracy: 0.4845 - val_loss: 0.8898 - val_accuracy: 0.4514\n","Epoch 33/50\n","7204/7204 [==============================] - 7s 916us/step - loss: 0.8790 - accuracy: 0.4939 - val_loss: 0.8895 - val_accuracy: 0.4721\n","Epoch 34/50\n","7204/7204 [==============================] - 7s 916us/step - loss: 0.8811 - accuracy: 0.4857 - val_loss: 0.8833 - val_accuracy: 0.4575\n","Epoch 35/50\n","7204/7204 [==============================] - 7s 920us/step - loss: 0.8789 - accuracy: 0.4976 - val_loss: 0.8876 - val_accuracy: 0.4503\n","Epoch 36/50\n","7204/7204 [==============================] - 7s 914us/step - loss: 0.8790 - accuracy: 0.4999 - val_loss: 0.8906 - val_accuracy: 0.4872\n","Epoch 37/50\n","7204/7204 [==============================] - 7s 918us/step - loss: 0.8773 - accuracy: 0.4996 - val_loss: 0.8853 - val_accuracy: 0.4615\n","Epoch 38/50\n","7204/7204 [==============================] - 7s 919us/step - loss: 0.8757 - accuracy: 0.5090 - val_loss: 0.8882 - val_accuracy: 0.4676\n","Epoch 39/50\n","7204/7204 [==============================] - 7s 915us/step - loss: 0.8732 - accuracy: 0.5101 - val_loss: 0.8923 - val_accuracy: 0.4749\n","Epoch 40/50\n","7204/7204 [==============================] - 7s 910us/step - loss: 0.8744 - accuracy: 0.5068 - val_loss: 0.8897 - val_accuracy: 0.4564\n","Epoch 41/50\n","7204/7204 [==============================] - 7s 917us/step - loss: 0.8687 - accuracy: 0.5215 - val_loss: 0.9025 - val_accuracy: 0.4676\n","Epoch 42/50\n","7204/7204 [==============================] - 7s 917us/step - loss: 0.8677 - accuracy: 0.5218 - val_loss: 0.8954 - val_accuracy: 0.4927\n","Epoch 43/50\n","7204/7204 [==============================] - 7s 920us/step - loss: 0.8642 - accuracy: 0.5337 - val_loss: 0.8965 - val_accuracy: 0.4637\n","Epoch 44/50\n","7204/7204 [==============================] - 7s 915us/step - loss: 0.8618 - accuracy: 0.5262 - val_loss: 0.9009 - val_accuracy: 0.4553\n","Epoch 45/50\n","7204/7204 [==============================] - 7s 926us/step - loss: 0.8586 - accuracy: 0.5283 - val_loss: 0.8979 - val_accuracy: 0.4899\n","Epoch 46/50\n","7204/7204 [==============================] - 7s 913us/step - loss: 0.8574 - accuracy: 0.5373 - val_loss: 0.8937 - val_accuracy: 0.4927\n","Epoch 47/50\n","7204/7204 [==============================] - 7s 918us/step - loss: 0.8524 - accuracy: 0.5414 - val_loss: 0.9066 - val_accuracy: 0.4648\n","Epoch 48/50\n","7204/7204 [==============================] - 7s 918us/step - loss: 0.8462 - accuracy: 0.5459 - val_loss: 0.8987 - val_accuracy: 0.4642\n","Epoch 49/50\n","7204/7204 [==============================] - 7s 913us/step - loss: 0.8415 - accuracy: 0.5612 - val_loss: 0.9158 - val_accuracy: 0.4615\n","Epoch 50/50\n","7204/7204 [==============================] - 7s 916us/step - loss: 0.8363 - accuracy: 0.5691 - val_loss: 0.9077 - val_accuracy: 0.4793\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fcbbfb32c88>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"gP0WFPOjZjHp","colab_type":"text"},"source":["Vemos que aunque aumentemos el número de épocas nuestro modelo se comporta prácticamente igual.\n","\n","Llegados a este punto en el que estamos haciendo bastantes pruebas de forma manual, vamos a usar Hyper-opt para optimizar nuestros resultados y encontrar las características del modelo que mejor prestaciones nos de."]},{"cell_type":"code","metadata":{"id":"_aXJacp2ZiQi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"ok","timestamp":1593238807295,"user_tz":-120,"elapsed":11700,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"4c1442d3-6f2d-4ca0-a109-c792e68ef33d"},"source":["# instalamos los paquetes necesarios\n","!pip install networkx==1.11 # para instala hyperopt correctamente, si no, da errores\n","!pip install hyperopt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting networkx==1.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2c/e473e54afc9fae58dfa97066ef6709a7e35a1dd1c28c5a3842989322be00/networkx-1.11-py2.py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from networkx==1.11) (4.4.2)\n","\u001b[31mERROR: scikit-image 0.16.2 has requirement networkx>=2.0, but you'll have networkx 1.11 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: networkx\n","  Found existing installation: networkx 2.4\n","    Uninstalling networkx-2.4:\n","      Successfully uninstalled networkx-2.4\n","Successfully installed networkx-1.11\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.11)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7W-3l-cRBDDc","colab_type":"text"},"source":["Vamos a trabajar con 4 variables:\n","\n","- el número de filtros en las capas convolucionales\n","- el porcentaje de dropout\n","- el número de neuronas en la capa densa\n","- el tamaño del kernel en las capas convolucionales"]},{"cell_type":"code","metadata":{"id":"s3kiDjUFX4EL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"ok","timestamp":1593240124376,"user_tz":-120,"elapsed":209896,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"6b8ed4c4-7de4-49de-a002-8f0acb009602"},"source":["# imports necesarios\n","import sys\n","import time\n","from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","from keras.callbacks import EarlyStopping\n","\n","\n","# definimos nuestro espacio de búsqueda\n","# vamos a variar:\n","# - el número de filtros en nuestras capas conv\n","# - el porcentaje de dropout\n","# - el número de neuronas en la capa dense\n","# - el tamaño del kernel en las capas conv\n","space = {\n","    'n_filters_conv': hp.choice('n_filters_conv', [32, 64, 128]),\n","    'dropout': hp.uniform('dropout', 0.0, 0.5),\n","    'neurons_dense': hp.choice('neurons_dense', [256, 512, 1024]), \n","    'kernel_size' : hp.choice('kernel_size', [3, 5])\n","}\n","\n","def\tget_callbacks(pars):\n","  callbacks\t= [EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=0, mode='auto')]\n","  return callbacks\n","\n","def mi_cnn(pars):\n","  print ('Parameters: ', pars)\n","  # Inizializamos el modelo\n","  model = Sequential()\n","\n","  # Definimos una capa convolucional\n","  model.add(Conv2D(pars['n_filters_conv'], kernel_size=(pars['kernel_size']), input_shape=(64, 64, 3)))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(pars['dropout']))\n","\n","  # Definimos una segunda capa convolucional\n","  model.add(Conv2D(pars['n_filters_conv'], kernel_size=(pars['kernel_size']), activation='relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(pars['dropout']))\n","\n","  # Definimos una tercera capa convolucional\n","  model.add(Conv2D(pars['n_filters_conv'], kernel_size=(pars['kernel_size']), activation='relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(pars['dropout']))\n","\n","  # Añadimos nuestro clasificador\n","  model.add(Flatten())\n","  model.add(Dense(pars['neurons_dense'], activation='relu', kernel_constraint=max_norm(3.)))\n","  model.add(Dropout(pars['dropout']))\n","  model.add(Dense(num_classes, activation='softmax'))\n","\n","  # Compilamos el modelo\n","  model.compile(loss='categorical_crossentropy',\n","                optimizer=Adam(lr=0.0001, decay=1e-6),\n","                metrics=['accuracy'])\n","\n","  # Entrenamos el modelo\n","  history = model.fit(train_imgs_loaded, Ytrain,\n","            batch_size=128,\n","            shuffle=True,\n","            epochs=50,\n","            validation_data=(val_imgs_loaded, Yval),\n","            verbose = 0,\n","            callbacks = get_callbacks(pars)) \n","  \n","\n","  best_epoch_loss = np.argmin(history.history['val_loss'])\n","  best_val_loss = np.min(history.history['val_loss'])\n","  best_val_acc = np.max(history.history['val_accuracy'])\n","  \n","  print('Epoch {} - val acc: {} - val loss: {}'.format(best_epoch_loss, best_val_acc, best_val_loss))\n","  sys.stdout.flush()\n","  \n","  return {'loss': best_val_loss, 'best_epoch': best_epoch_loss, 'eval_time': time.time(), 'status': STATUS_OK, 'model': model, 'history': history}\n","\n","\n","trials = Trials()\n","best = fmin(mi_cnn, space, algo=tpe.suggest, max_evals=10, trials=trials)\n","print(best)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Parameters: \n","{'dropout': 0.007405141224077039, 'kernel_size': 5, 'n_filters_conv': 64, 'neurons_dense': 512}\n","Epoch 2 - val acc: 0.4949720799922943 - val loss: 0.8708138468545242\n","Parameters: \n","{'dropout': 0.15950979926671216, 'kernel_size': 3, 'n_filters_conv': 128, 'neurons_dense': 512}\n","Epoch 0 - val acc: 0.4949720799922943 - val loss: 0.8738649612698475\n","Parameters: \n","{'dropout': 0.2185707044649131, 'kernel_size': 5, 'n_filters_conv': 32, 'neurons_dense': 512}\n","Epoch 3 - val acc: 0.4949720799922943 - val loss: 0.8767173019867369\n","Parameters: \n","{'dropout': 0.0030657473607762498, 'kernel_size': 3, 'n_filters_conv': 128, 'neurons_dense': 1024}\n","Epoch 1 - val acc: 0.4949720799922943 - val loss: 0.8731627184585486\n","Parameters: \n","{'dropout': 0.12805414951968475, 'kernel_size': 5, 'n_filters_conv': 128, 'neurons_dense': 1024}\n","Epoch 3 - val acc: 0.4949720799922943 - val loss: 0.8743143003080144\n","Parameters: \n","{'dropout': 0.3913452147594918, 'kernel_size': 3, 'n_filters_conv': 32, 'neurons_dense': 1024}\n","Epoch 4 - val acc: 0.4949720799922943 - val loss: 0.9058789199956968\n","Parameters: \n","{'dropout': 0.007079212751563035, 'kernel_size': 3, 'n_filters_conv': 64, 'neurons_dense': 1024}\n","Epoch 4 - val acc: 0.4949720799922943 - val loss: 0.8709334216304332\n","Parameters: \n","{'dropout': 0.3033282294448333, 'kernel_size': 3, 'n_filters_conv': 32, 'neurons_dense': 512}\n","Epoch 0 - val acc: 0.4949720799922943 - val loss: 0.8910828931371593\n","Parameters: \n","{'dropout': 0.3937249972066476, 'kernel_size': 5, 'n_filters_conv': 128, 'neurons_dense': 256}\n","Epoch 0 - val acc: 0.4949720799922943 - val loss: 0.8743990836196771\n","Parameters: \n","{'dropout': 0.08923659856707372, 'kernel_size': 5, 'n_filters_conv': 64, 'neurons_dense': 512}\n","Epoch 0 - val acc: 0.4949720799922943 - val loss: 0.8733590950513019\n","100%|██████████| 10/10 [03:29<00:00, 20.93s/it, best loss: 0.8708138468545242]\n","{'dropout': 0.007405141224077039, 'kernel_size': 1, 'n_filters_conv': 1, 'neurons_dense': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yefHyTAEhshQ","colab_type":"text"},"source":["Viendo los resultados no superamos el 49% de accuracy en validación, así que vamos a tomarlo como definitivo y pasamos a evaluar nuestro modelo final.\n","\n","{'dropout': 0.007405141224077039, 'kernel_size': 1, 'n_filters_conv': 1, 'neurons_dense': 1}\n","\n","Parameters: \n","{'dropout': 0.007405141224077039, 'kernel_size': 5, 'n_filters_conv': 64, 'neurons_dense': 512}\n","Epoch 2 - val acc: 0.4949720799922943 - val loss: 0.8708138468545242\n","\n","64 filtros, 512 neuronas y 5x5 el kernel."]},{"cell_type":"code","metadata":{"id":"TM90tSDZX4Ax","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"ok","timestamp":1593241178555,"user_tz":-120,"elapsed":41303,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"86730dc0-3109-4e12-96a3-bfde4f4605a2"},"source":["# Inizializamos el modelo definitivo con los parámetros optimizados\n","modelDef = Sequential()\n","\n","# Definimos una capa convolucional\n","modelDef.add(Conv2D(64, kernel_size=(5, 5), input_shape=(64, 64, 3)))\n","modelDef.add(MaxPooling2D(pool_size=(2, 2)))\n","modelDef.add(Dropout(0.00745))\n","\n","# Definimos una segunda capa convolucional\n","modelDef.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n","modelDef.add(MaxPooling2D(pool_size=(2, 2)))\n","modelDef.add(Dropout(0.00745))\n","\n","# Definimos una tercera capa convolucional\n","modelDef.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n","modelDef.add(MaxPooling2D(pool_size=(2, 2)))\n","modelDef.add(Dropout(0.00745))\n","\n","# Añadimos nuestro clasificador\n","modelDef.add(Flatten())\n","modelDef.add(Dense(1024, activation='relu', kernel_constraint=max_norm(3.)))\n","modelDef.add(Dropout(0.0074))\n","modelDef.add(Dense(num_classes, activation='softmax'))\n","\n","# Compilamos el modelo\n","modelDef.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=0.0001, decay=1e-6),\n","              metrics=['accuracy'])\n","\n","# Entrenamos el modelo\n","modelDef.fit(train_imgs_loaded, Ytrain,\n","          batch_size=128,\n","          shuffle=True,\n","          epochs=10,\n","          validation_data=(val_imgs_loaded, Yval)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 7204 samples, validate on 1790 samples\n","Epoch 1/10\n","7204/7204 [==============================] - 5s 662us/step - loss: 0.9584 - accuracy: 0.4647 - val_loss: 0.8761 - val_accuracy: 0.4950\n","Epoch 2/10\n","7204/7204 [==============================] - 4s 496us/step - loss: 0.8868 - accuracy: 0.4643 - val_loss: 0.8743 - val_accuracy: 0.4480\n","Epoch 3/10\n","7204/7204 [==============================] - 4s 494us/step - loss: 0.8870 - accuracy: 0.4663 - val_loss: 0.8752 - val_accuracy: 0.4480\n","Epoch 4/10\n","7204/7204 [==============================] - 4s 499us/step - loss: 0.8864 - accuracy: 0.4686 - val_loss: 0.8713 - val_accuracy: 0.4950\n","Epoch 5/10\n","7204/7204 [==============================] - 4s 494us/step - loss: 0.8864 - accuracy: 0.4646 - val_loss: 0.8712 - val_accuracy: 0.4950\n","Epoch 6/10\n","7204/7204 [==============================] - 4s 494us/step - loss: 0.8859 - accuracy: 0.4629 - val_loss: 0.8716 - val_accuracy: 0.4950\n","Epoch 7/10\n","7204/7204 [==============================] - 4s 497us/step - loss: 0.8856 - accuracy: 0.4703 - val_loss: 0.8826 - val_accuracy: 0.4480\n","Epoch 8/10\n","7204/7204 [==============================] - 4s 495us/step - loss: 0.8857 - accuracy: 0.4615 - val_loss: 0.8735 - val_accuracy: 0.4950\n","Epoch 9/10\n","7204/7204 [==============================] - 4s 494us/step - loss: 0.8852 - accuracy: 0.4638 - val_loss: 0.8714 - val_accuracy: 0.4950\n","Epoch 10/10\n","7204/7204 [==============================] - 4s 495us/step - loss: 0.8851 - accuracy: 0.4756 - val_loss: 0.8751 - val_accuracy: 0.4950\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fcbbc0b0c18>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"j7VpevKNBz_E","colab_type":"text"},"source":["Por último evaluamos el modelo con el conjunto de test."]},{"cell_type":"code","metadata":{"id":"xnXJIhQWOKI8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1593241240823,"user_tz":-120,"elapsed":1202,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"ed66f5a3-17c1-458e-d22e-f6ea278dc692"},"source":["# Evaluamos el modelo\n","scores = modelDef.evaluate(test_imgs_loaded, Ytest)\n","\n","print('Loss: %.3f' % scores[0])\n","print('Accuracy: %.3f' % scores[1])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2277/2277 [==============================] - 1s 359us/step\n","Loss: 0.895\n","Accuracy: 0.477\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X4UbKyMCwTMC","colab_type":"text"},"source":["**CONCLUSIÓN**\n","\n","No me quedo muy conforme con el resultado del 47% o 49% de accuracy. Se puede deber a la baja calidad de las imágenes (importante la limitación de haber reducido tanto el tamaño), a la dificultad del dataset y al desbalanceo del mismo. Aunque no descartaría algún error mío en el procesamiento. Por más que lo he revisado no he encontrado nada.\n","\n","Viendo las predicciones (abajo está copiado un ejemplo) se observa como siempre queda casi al 50% entre los pisos baratos y medios, y esto podría tener su explicación en que se trata de un dataset muy desbalanceado donde hay pocos pisos caros.\n","\n","\n","\n","array([[0.47080284, 0.46632615, 0.06287099],\n","       [0.47080284, 0.46632615, 0.06287099],\n","       [0.47080284, 0.46632615, 0.06287099],\n","       [0.47080284, 0.46632615, 0.06287099],\n","       [0.47080284, 0.46632615, 0.06287099],"]},{"cell_type":"code","metadata":{"id":"B3648oAiw3jN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}