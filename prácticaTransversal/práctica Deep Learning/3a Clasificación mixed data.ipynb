{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3a Clasificación mixed data.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1nfttdI5qRRN5QOWDR8PYJEXrvf7gSnFB","authorship_tag":"ABX9TyOnfw40YgGaKFj7bqj4iyCm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CF3PrvWJ6X8t","colab_type":"text"},"source":["**CLASIFICACIÓN PARTIENDO DE IMÁGENES Y DATOS NUMÉRICOS Y CATEGÓRICOS**\n","\n","En este notebook vamos a tratar de clasificar los apartamentos según su precio en baratos, medios o caros. Para ello estableceremos los límites en 50€ para los baratos y 150€ para los caros.\n","Dicha predicción se va a hacer a partir de dos fuentes de datos distintas: por un lado las imágenes que tenemos en el dataset de airbnb que venimos usando en las prácticas de este Bootcamp y por otro los datos numéricos y categóricos de dicho dataset.\n","\n","En primer lugar nos descargamos el fichero de internet y lo copiamos en un directorio local de My Drive donde tenemos recogido todo el entorno de esta práctica.\n","A continuación hacemos lo mismo con las imágenes de cada una de las entradas. Usamos la vista en miniatura que sacamos de la URL de dicho fichero.\n","\n","También montamos el google collab con My Drive para tenerlo vinculado.\n","\n","Estos pasos solo hay que realizarlos la primera vez, una vez que tenemos los ficheros en My Drive se pueden saltar y pasamos a cargar los datos directamente desde dicho directorio."]},{"cell_type":"code","metadata":{"id":"yGAWhUGP6nRG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1593064923489,"user_tz":-120,"elapsed":62620,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"a49fee41-1611-48e3-9bd8-5cb4329ec326"},"source":["# nos descargamos el dataset de OpenDataSoft\n","!wget -O \"airbnb-listings.csv\" \"https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\"\n","\n","!ls -lah"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-25 06:01:03--  https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\n","Resolving public.opendatasoft.com (public.opendatasoft.com)... 34.249.199.226, 34.248.20.69\n","Connecting to public.opendatasoft.com (public.opendatasoft.com)|34.249.199.226|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/csv]\n","Saving to: ‘airbnb-listings.csv’\n","\n","airbnb-listings.csv     [  <=>               ]  54.19M  2.77MB/s    in 50s     \n","\n","2020-06-25 06:02:01 (1.09 MB/s) - ‘airbnb-listings.csv’ saved [56826824]\n","\n","total 55M\n","drwxr-xr-x 1 root root 4.0K Jun 25 06:01 .\n","drwxr-xr-x 1 root root 4.0K Jun 25 05:59 ..\n","-rw-r--r-- 1 root root  55M Jun 25 06:02 airbnb-listings.csv\n","drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n","drwx------ 4 root root 4.0K Jun 25 06:00 drive\n","drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-4dDxIX4wNAb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1593148776645,"user_tz":-120,"elapsed":8497,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"d123b864-a9b6-4071-b32b-17b8a663be88"},"source":["!ls -lah"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 16K\n","drwxr-xr-x 1 root root 4.0K Jun 17 16:18 .\n","drwxr-xr-x 1 root root 4.0K Jun 26 05:15 ..\n","drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n","drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YLSxaq6U32t_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27R_6DOoEpEF","colab_type":"code","colab":{}},"source":["!cp airbnb-listings.csv \"drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyUGFt37FHVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1593149243426,"user_tz":-120,"elapsed":875,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"49df4fd7-e4e7-4e33-e62f-2b9c7e4ff32f"},"source":["# aquí creamos nuestra estructura de datos, que va a consistir en la url de la\n","# imagen y un índice para saber donde insertarla en nuestro array\n","images_paths = [[i, img_url] for i, img_url in enumerate(full_df['Thumbnail Url'])]\n","images_paths[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0,\n","  'https://a0.muscache.com/im/pictures/cffe393a-0d84-4fd5-ab4c-a62e067c1b0d.jpg?aki_policy=small'],\n"," [1,\n","  'https://a0.muscache.com/im/pictures/ea919e56-aa99-4d5d-a129-1edf0d117d6a.jpg?aki_policy=small'],\n"," [2,\n","  'https://a0.muscache.com/im/pictures/57011236/eea5c213_original.jpg?aki_policy=small'],\n"," [3,\n","  'https://a0.muscache.com/im/pictures/974f0245-55c2-4e8c-b9bf-14c1c975c798.jpg?aki_policy=small'],\n"," [4,\n","  'https://a0.muscache.com/im/pictures/c2dde263-20dd-43af-8c6b-be636c2c0ce1.jpg?aki_policy=small']]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"My93mt1qMtcv","colab_type":"code","colab":{}},"source":["import imageio as io\n","import cv2\n","\n","# esta es la función que se descargará la imagen y devolverá la imagen y el \n","# índice indicando la posición donde se incrustará la imagen en nuestro array\n","def get_image(data_url, target_size=(224, 224)):\n","    idx, url = data_url\n","    try:\n","        img = io.imread(url)\n","        # hay alguna imagen en blanco y negro y daría error al incluirla en \n","        # nuestro array de imagenes que tiene 3 canales, así que convertimos\n","        # todas las imágenes que tengan menos de 3 dimensiones a color\n","        if img.ndim < 3:\n","            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","        img = cv2.resize(img, dsize=target_size)\n","        return img, idx\n","    except IOError as err:\n","        return (None, idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WldeZWd2NAbm","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# en este array iremos incrustando las imágenes conforme las vayamos obteniendo\n","loaded_images = np.zeros((len(images_paths), 224, 224, 3), dtype=np.uint8)\n","\n","# y en este array llevaremos un control de cuales se han cargado correctamente\n","# y cuales no\n","was_loaded = np.zeros(len(images_paths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4PSNB_hNVLC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593151722336,"user_tz":-120,"elapsed":498858,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"e1f758ce-a160-4e1e-8b4b-0e7fdf5ac77e"},"source":["import concurrent\n","from tqdm import tqdm\n","\n","# creamos un pool de procesos que se irán descargando las imágenes\n","# por defecto, se crearán tantos como CPUs tenga vuestra máquina\n","with concurrent.futures.ProcessPoolExecutor() as executor:\n","    # procesamos la lista de urls de imágenes paralelizandola con el pool de procesos\n","    for (img, idx) in tqdm(executor.map(get_image, images_paths), total=len(images_paths)):\n","        # metemos la imagen en nuestro array\n","        if img is not None:\n","            loaded_images[idx] = img\n","            was_loaded[idx] = 1\n","        else:\n","            was_loaded[idx] = 0\n","\n","print('Terminado!')\n","print(f'Total de imágenes recuperadas correctamente: {sum(was_loaded)}/{len(images_paths)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 14001/14001 [08:14<00:00, 28.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Terminado!\n","Total de imágenes recuperadas correctamente: 11271.0/14001\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"INHNB-WhNsvG","colab_type":"code","colab":{}},"source":["# guardamos las imágenes (y yo os recomiendo que os lo guardéis en GDrive para evitar tener que repetir esto)\n","np.save('images.npy', loaded_images)\n","np.save('was_loaded.npy', was_loaded)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4-TANTHNsWk","colab_type":"code","colab":{}},"source":["# almacenamos las imagenes en nuestro drive\n","!cp images.npy \"drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning\"\n","!cp was_loaded.npy \"drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJu_2Fcm8qNz","colab_type":"text"},"source":["Esta parte de descarga, montado y copiado solo hace falta ejecutarla la primera vez. Una vez que lo tenemos copiado solo necesitamos cargarlo directamente.\n","\n","A partir de aquí empieza nuestro ejercicio de clasificación.\n","\n","Como hábito de buena costumbre, para no incurrir en errores involuntarios, en primer lugar se va a dividir el dataset original en train, validation y test.\n","\n","Se trabaja únicamente con el de train con el objetivo de elegir un modelo. Eso se verifica con el conjunto de validation y finalmente se aplica ese \"entrenamiento\" al bloque de test."]},{"cell_type":"code","metadata":{"id":"E63PIJHrVKuF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593344048997,"user_tz":-120,"elapsed":767,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"9c17cb6b-d84e-4e80-ac60-b7a8daa8fbe4"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GQK8_P3qVKOb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1593344061734,"user_tz":-120,"elapsed":13493,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"a28612df-bc32-4b90-f2f0-0f486c70e9b1"},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","%matplotlib inline\n","cm = plt.cm.RdBu\n","cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","#hacemos la divisón en train, val y test\n","full_df = pd.read_csv('drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning/airbnb-listings.csv', sep=';', decimal='.')\n","full_train, test = train_test_split(full_df, test_size=0.2, shuffle=True, random_state=0)\n","train, val = train_test_split(full_train, test_size=0.2, shuffle=True, random_state=0)\n","\n","print(f'Dimensiones del dataset de training: {train.shape}')\n","print(f'Dimensiones del dataset de validación: {val.shape}')\n","print(f'Dimensiones del dataset de test: {test.shape}')\n","\n","# Guardamos\n","train.to_csv('./train.csv', sep=';', decimal='.', index=True)\n","val.to_csv('./val.csv', sep=';', decimal='.', index=True)\n","test.to_csv('./test.csv', sep=';', decimal='.', index=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (8960, 89)\n","Dimensiones del dataset de validación: (2240, 89)\n","Dimensiones del dataset de test: (2801, 89)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"snh4Brs5SLwN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1593344143670,"user_tz":-120,"elapsed":27565,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"6a2e3640-25dc-41b8-bedd-462fa1138482"},"source":["#cargamos las imágenes desde el directorio de My Drive (ya las habíamos descargado previamente)\n","images  = np.load('drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning/images.npy')\n","was_loaded  = np.load('drive/My Drive/BootcampBD&ML/práctica/prácticaDeepLearning/was_loaded.npy')\n","\n","#cargamos los datos ya divididos en train, val y test\n","df_train = pd.read_csv('./train.csv', sep=';', decimal='.')\n","df_val = pd.read_csv('./val.csv', sep=';', decimal='.')\n","df_test = pd.read_csv('./test.csv', sep=';', decimal='.')\n","\n","#usando el índice de la división anterior obtenemos los conjuntos de test, val y test en las imágenes\n","train_imgs = images[df_train['Unnamed: 0']]\n","val_imgs = images[df_val['Unnamed: 0']]\n","test_imgs = images[df_test['Unnamed: 0']]\n","\n","train_was_loaded = was_loaded[df_train['Unnamed: 0']]\n","val_was_loaded = was_loaded[df_val['Unnamed: 0']]\n","test_was_loaded = was_loaded[df_test['Unnamed: 0']]\n","\n","print(f'Dimensiones del dataset de training: {train_imgs.shape}')\n","print(f'Dimensiones del dataset de validación: {val_imgs.shape}')\n","print(f'Dimensiones del dataset de test: {test_imgs.shape}')\n","\n","print(f'Dimensiones del dataset de training: {train_was_loaded.shape}')\n","print(f'Dimensiones del dataset de validación: {val_was_loaded.shape}')\n","print(f'Dimensiones del dataset de test: {test_was_loaded.shape}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (8960, 224, 224, 3)\n","Dimensiones del dataset de validación: (2240, 224, 224, 3)\n","Dimensiones del dataset de test: (2801, 224, 224, 3)\n","Dimensiones del dataset de training: (8960,)\n","Dimensiones del dataset de validación: (2240,)\n","Dimensiones del dataset de test: (2801,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oveWlY0PQu8G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1593344144981,"user_tz":-120,"elapsed":21650,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"27ec1379-8b01-4b1e-b4ec-ed7b7c498863"},"source":["# nos quedamos con los datos e imágenes para los que hemos podido encontrar imágenes\n","train_imgs_loaded = train_imgs[train_was_loaded == 1]\n","val_imgs_loaded = val_imgs[val_was_loaded == 1]\n","test_imgs_loaded = test_imgs[test_was_loaded == 1]\n","\n","train_with_imgs = df_train[train_was_loaded == 1]\n","val_with_imgs = df_val[val_was_loaded == 1]\n","test_with_imgs = df_test[test_was_loaded == 1]\n","\n","print(f'Dimensiones del dataset de training: {train_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de validación: {val_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de test: {test_imgs_loaded.shape}')\n","\n","print(f'Dimensiones del dataset de training: {train_with_imgs.shape}')\n","print(f'Dimensiones del dataset de validación: {val_with_imgs.shape}')\n","print(f'Dimensiones del dataset de test: {test_with_imgs.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (7204, 224, 224, 3)\n","Dimensiones del dataset de validación: (1790, 224, 224, 3)\n","Dimensiones del dataset de test: (2277, 224, 224, 3)\n","Dimensiones del dataset de training: (7204, 90)\n","Dimensiones del dataset de validación: (1790, 90)\n","Dimensiones del dataset de test: (2277, 90)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R7ZiZcPXNRVG","colab_type":"code","colab":{}},"source":["#definimos la función de procesado de datos categóricos y numéricos\n","def preprocesado(train, val, test):\n","  \n","  #eliminamos las columnas que no aportan\n","  train.drop(['ID','Scrape ID','Last Scraped','Host ID','Calendar last Scraped','Listing Url','Thumbnail Url',\n","         'Medium Url','Picture Url','XL Picture Url','Host URL','Host Thumbnail Url','Host Picture Url',\n","        'Name','Summary','Space','Description','Neighborhood Overview','Notes','Transit','Access',\n","         'Interaction','House Rules','Host Name','Host About','Street','Host Location','State','Market',\n","         'Smart Location','Country Code','Country','Geolocation','Weekly Price','Monthly Price',\n","         'Host Acceptance Rate','Experiences Offered','Has Availability','License','Jurisdiction Names','Square Feet','City'], \n","        axis=1, inplace=True)\n","  val.drop(['ID','Scrape ID','Last Scraped','Host ID','Calendar last Scraped','Listing Url','Thumbnail Url',\n","         'Medium Url','Picture Url','XL Picture Url','Host URL','Host Thumbnail Url','Host Picture Url',\n","        'Name','Summary','Space','Description','Neighborhood Overview','Notes','Transit','Access',\n","         'Interaction','House Rules','Host Name','Host About','Street','Host Location','State','Market',\n","         'Smart Location','Country Code','Country','Geolocation','Weekly Price','Monthly Price',\n","         'Host Acceptance Rate','Experiences Offered','Has Availability','License','Jurisdiction Names','Square Feet','City'], \n","        axis=1, inplace=True)\n","  test.drop(['ID','Scrape ID','Last Scraped','Host ID','Calendar last Scraped','Listing Url','Thumbnail Url',\n","         'Medium Url','Picture Url','XL Picture Url','Host URL','Host Thumbnail Url','Host Picture Url',\n","        'Name','Summary','Space','Description','Neighborhood Overview','Notes','Transit','Access',\n","         'Interaction','House Rules','Host Name','Host About','Street','Host Location','State','Market',\n","         'Smart Location','Country Code','Country','Geolocation','Weekly Price','Monthly Price',\n","         'Host Acceptance Rate','Experiences Offered','Has Availability','License','Jurisdiction Names','Square Feet','City'], \n","        axis=1, inplace=True)\n","  \n","  #PRICE\n","  #imputamos valores vacíos con la media de train\n","  MeanPriceTrain = train['Price'].mean()\n","  train['Price'].fillna(MeanPriceTrain, inplace=True)\n","  val['Price'].fillna(MeanPriceTrain, inplace=True)\n","  test['Price'].fillna(MeanPriceTrain, inplace=True)\n","  #transformamos variable Price a gausiana\n","  train['Price'] = train['Price'].apply(lambda x: np.log10(x))\n","  val['Price'] = val['Price'].apply(lambda x: np.log10(x))\n","  test['Price'] = test['Price'].apply(lambda x: np.log10(x))\n","  #categorizamos la variable precio en 3 tipos: barato (0), medio (1) y caro (2).\n","  train['Cat_Price'] = train['Price'].apply(lambda x: 0 if x < np.log10(50) else (1 if x < np.log10(150) else 2))\n","  val['Cat_Price'] = val['Price'].apply(lambda x: 0 if x < np.log10(50) else (1 if x < np.log10(150) else 2))\n","  test['Cat_Price'] = test['Price'].apply(lambda x: 0 if x < np.log10(50) else (1 if x < np.log10(150) else 2))\n","  \n","  \n","  #FECHAS\n","  train['Host Since'] = pd.to_datetime(train['Host Since'], format=\"%Y-%m-%d\")\n","  train['First Review'] = pd.to_datetime(train['First Review'], format=\"%Y-%m-%d\")\n","  train['Last Review'] = pd.to_datetime(train['Last Review'], format=\"%Y-%m-%d\")\n","  train['Host Since'] = train['Host Since'].apply(lambda x: 2017 - x.year)\n","  train['First Review'] = train['First Review'].apply(lambda x: 2017 - x.year)\n","  train['Last Review'] = train['Last Review'].apply(lambda x: 2017 - x.year)\n","\n","  val['Host Since'] = pd.to_datetime(val['Host Since'], format=\"%Y-%m-%d\")\n","  val['First Review'] = pd.to_datetime(val['First Review'], format=\"%Y-%m-%d\")\n","  val['Last Review'] = pd.to_datetime(val['Last Review'], format=\"%Y-%m-%d\")\n","  val['Host Since'] = val['Host Since'].apply(lambda x: 2017 - x.year)\n","  val['First Review'] = val['First Review'].apply(lambda x: 2017 - x.year)\n","  val['Last Review'] = val['Last Review'].apply(lambda x: 2017 - x.year)\n","\n","  test['Host Since'] = pd.to_datetime(test['Host Since'], format=\"%Y-%m-%d\")\n","  test['First Review'] = pd.to_datetime(test['First Review'], format=\"%Y-%m-%d\")\n","  test['Last Review'] = pd.to_datetime(test['Last Review'], format=\"%Y-%m-%d\")\n","  test['Host Since'] = test['Host Since'].apply(lambda x: 2017 - x.year)\n","  test['First Review'] = test['First Review'].apply(lambda x: 2017 - x.year)\n","  test['Last Review'] = test['Last Review'].apply(lambda x: 2017 - x.year)\n","\n","  #Imputamos valores en variables categóricas donde tomamos la moda para los valores que faltan.\n","  #Lo extraemos en una variable disinta para cada columna con la intención de aplicar el mismo valor en val y test\n","  ModeHSTrain = train['Host Since'].mode()[0]\n","  ModeHLCTrain = train['Host Listings Count'].mode()[0]\n","  ModeHTLCTrain = train['Host Total Listings Count'].mode()[0]\n","  ModeBathroomsTrain = train['Bathrooms'].mode()[0]\n","  ModeBedroomsTrain = train['Bedrooms'].mode()[0]\n","  ModeBedsTrain = train['Beds'].mode()[0]\n","\n","  train['Host Since'].fillna(ModeHSTrain, inplace=True)\n","  train['Host Listings Count'].fillna(ModeHLCTrain, inplace=True)\n","  train['Host Total Listings Count'].fillna(ModeHTLCTrain, inplace=True)\n","  train['Bathrooms'].fillna(ModeBathroomsTrain, inplace=True)\n","  train['Bedrooms'].fillna(ModeBedroomsTrain, inplace=True)\n","  train['Beds'].fillna(ModeBedsTrain, inplace=True)\n","\n","  val['Host Since'].fillna(ModeHSTrain, inplace=True)\n","  val['Host Listings Count'].fillna(ModeHLCTrain, inplace=True)\n","  val['Host Total Listings Count'].fillna(ModeHTLCTrain, inplace=True)\n","  val['Bathrooms'].fillna(ModeBathroomsTrain, inplace=True)\n","  val['Bedrooms'].fillna(ModeBedroomsTrain, inplace=True)\n","  val['Beds'].fillna(ModeBedsTrain, inplace=True)\n","  test['Host Since'].fillna(ModeHSTrain, inplace=True)\n","  test['Host Listings Count'].fillna(ModeHLCTrain, inplace=True)\n","  test['Host Total Listings Count'].fillna(ModeHTLCTrain, inplace=True)\n","  test['Bathrooms'].fillna(ModeBathroomsTrain, inplace=True)\n","  test['Bedrooms'].fillna(ModeBedroomsTrain, inplace=True)\n","  test['Beds'].fillna(ModeBedsTrain, inplace=True)\n","\n","  #Imputamos valores en variables lineales donde tomamos la media para los valores que faltan\n","  #Lo extraemos en una variable disinta para cada columna con la intención de aplicar el mismo valor en val y test\n","  MeanRSRatingTrain = train['Review Scores Rating'].mean()\n","  MeanRSAccuracyTrain = train['Review Scores Accuracy'].mean()\n","  MeanRSCleanlinessTrain = train['Review Scores Cleanliness'].mean()\n","  MeanRSCheckinTrain = train['Review Scores Checkin'].mean()\n","  MeanRSCommunicationTrain = train['Review Scores Communication'].mean()\n","  MeanRSLocationTrain = train['Review Scores Location'].mean()\n","  MeanRSValueTrain = train['Review Scores Value'].mean()\n","\n","  train['Review Scores Rating'].fillna(MeanRSRatingTrain, inplace=True)\n","  train['Review Scores Accuracy'].fillna(MeanRSAccuracyTrain, inplace=True)\n","  train['Review Scores Cleanliness'].fillna(MeanRSCleanlinessTrain, inplace=True)\n","  train['Review Scores Checkin'].fillna(MeanRSCheckinTrain, inplace=True)\n","  train['Review Scores Communication'].fillna(MeanRSCommunicationTrain, inplace=True)\n","  train['Review Scores Location'].fillna(MeanRSLocationTrain, inplace=True)\n","  train['Review Scores Value'].fillna(MeanRSValueTrain, inplace=True)\n","  val['Review Scores Rating'].fillna(MeanRSRatingTrain, inplace=True)\n","  val['Review Scores Accuracy'].fillna(MeanRSAccuracyTrain, inplace=True)\n","  val['Review Scores Cleanliness'].fillna(MeanRSCleanlinessTrain, inplace=True)\n","  val['Review Scores Checkin'].fillna(MeanRSCheckinTrain, inplace=True)\n","  val['Review Scores Communication'].fillna(MeanRSCommunicationTrain, inplace=True)\n","  val['Review Scores Location'].fillna(MeanRSLocationTrain, inplace=True)\n","  val['Review Scores Value'].fillna(MeanRSValueTrain, inplace=True)\n","  test['Review Scores Rating'].fillna(MeanRSRatingTrain, inplace=True)\n","  test['Review Scores Accuracy'].fillna(MeanRSAccuracyTrain, inplace=True)\n","  test['Review Scores Cleanliness'].fillna(MeanRSCleanlinessTrain, inplace=True)\n","  test['Review Scores Checkin'].fillna(MeanRSCheckinTrain, inplace=True)\n","  test['Review Scores Communication'].fillna(MeanRSCommunicationTrain, inplace=True)\n","  test['Review Scores Location'].fillna(MeanRSLocationTrain, inplace=True)\n","  test['Review Scores Value'].fillna(MeanRSValueTrain, inplace=True)\n","\n","  #los vacíos los consideramos como desconocidos\n","  train['Host Neighbourhood'].fillna('Unknown', inplace=True)\n","  train['Host Verifications'].fillna('Unknown', inplace=True)\n","  train['Neighbourhood'].fillna('Unknown', inplace=True)\n","  train['Zipcode'].fillna('Unknown', inplace=True)\n","  train['Amenities'].fillna('Unknown', inplace=True)\n","  train['First Review'].fillna('Unknown', inplace=True)\n","  train['Last Review'].fillna('Unknown', inplace=True)\n","  val['Host Neighbourhood'].fillna('Unknown', inplace=True)\n","  val['Host Verifications'].fillna('Unknown', inplace=True)\n","  val['Neighbourhood'].fillna('Unknown', inplace=True)\n","  val['Zipcode'].fillna('Unknown', inplace=True)\n","  val['Amenities'].fillna('Unknown', inplace=True)\n","  val['First Review'].fillna('Unknown', inplace=True)\n","  val['Last Review'].fillna('Unknown', inplace=True)\n","  test['Host Neighbourhood'].fillna('Unknown', inplace=True)\n","  test['Host Verifications'].fillna('Unknown', inplace=True)\n","  test['Neighbourhood'].fillna('Unknown', inplace=True)\n","  test['Zipcode'].fillna('Unknown', inplace=True)\n","  test['Amenities'].fillna('Unknown', inplace=True)\n","  test['First Review'].fillna('Unknown', inplace=True)\n","  test['Last Review'].fillna('Unknown', inplace=True)\n","\n","  #consideramos que donde falta un valor es porque no existe, es decir, no hay respuesta o la tasa es 0€\n","  train['Host Response Time'].fillna('No response', inplace=True)\n","  train['Host Response Rate'].fillna(0, inplace=True)\n","  train['Security Deposit'].fillna(0, inplace=True)\n","  train['Cleaning Fee'].fillna(0, inplace=True)\n","  train['Reviews per Month'].fillna(0, inplace=True)\n","  val['Host Response Time'].fillna('No response', inplace=True)\n","  val['Host Response Rate'].fillna(0, inplace=True)\n","  val['Security Deposit'].fillna(0, inplace=True)\n","  val['Cleaning Fee'].fillna(0, inplace=True)\n","  val['Reviews per Month'].fillna(0, inplace=True)\n","  test['Host Response Time'].fillna('No response', inplace=True)\n","  test['Host Response Rate'].fillna(0, inplace=True)\n","  test['Security Deposit'].fillna(0, inplace=True)\n","  test['Cleaning Fee'].fillna(0, inplace=True)\n","  test['Reviews per Month'].fillna(0, inplace=True)\n","\n","  #transformaciones contando palabras. es algo muy sencillo, queda pendiente mejorarlo con técnicas NLP en el futuro\n","  train['Amenities'] = train['Amenities'].apply(lambda x: len(str(x).split(',')))\n","  train['Host Verifications'] = train['Host Verifications'].apply(lambda x: len(str(x).split(',')))\n","  train['Features'] = train['Features'].apply(lambda x: len(str(x).split(',')))\n","  val['Amenities'] = val['Amenities'].apply(lambda x: len(str(x).split(',')))\n","  val['Host Verifications'] = val['Host Verifications'].apply(lambda x: len(str(x).split(',')))\n","  val['Features'] = val['Features'].apply(lambda x: len(str(x).split(',')))\n","  test['Amenities'] = test['Amenities'].apply(lambda x: len(str(x).split(',')))\n","  test['Host Verifications'] = test['Host Verifications'].apply(lambda x: len(str(x).split(',')))\n","  test['Features'] = test['Features'].apply(lambda x: len(str(x).split(',')))\n","\n","  #MeanEncoder\n","  categorical = ['Host Response Time', 'Host Neighbourhood', 'Neighbourhood','Neighbourhood Cleansed',\n","               'Neighbourhood Group Cleansed','Zipcode','Property Type','Room Type','Bed Type',\n","               'Calendar Updated','First Review','Last Review','Cancellation Policy']\n","  # En train creamos un dict para usarlo después en val y test\n","  mean_map = {}\n","  for c in categorical:\n","      mean = train.groupby(c)['Price'].mean()\n","      train[c] = train[c].map(mean)    \n","      mean_map[c] = mean\n","  for c in categorical:\n","    val[c] = val[c].map(mean_map[c])\n","  for c in categorical:\n","    test[c] = test[c].map(mean_map[c])\n"," #los valores vacíos de val y test los completo con la moda de train\n","  for c in categorical:\n","    val[c].fillna(train[c].mode()[0], inplace=True)\n","  for c in categorical:\n","    test[c].fillna(train[c].mode()[0], inplace=True)\n","\n","  #eliminamos la variable Price (ya está categorizada y ya se ha usado para el MeanEncoder)\n","  train.drop(['Price'],axis=1, inplace=True)\n","  val.drop(['Price'],axis=1, inplace=True)\n","  test.drop(['Price'],axis=1, inplace=True)\n","  \n","  #separamos las variables de entrada de la variable objetivo\n","  cols = train.columns.tolist()\n","  Xtrain = train[cols[0:-1]]\n","  Xval = val[cols[0:-1]]\n","  Xtest = test[cols[0:-1]]\n","\n","  #escalamos los valores de entrada\n","  cs = MinMaxScaler()\n","  Xtrain_Scaled = cs.fit_transform(Xtrain)\n","  Xval_Scaled = cs.transform(Xval)\n","  Xtest_Scaled = cs. transform(Xtest)\n","\n","  Ytrain = train[cols[-1]]\n","  Yval = val[cols[-1]]\n","  Ytest = test[cols[-1]]\n","\n","  return (Xtrain_Scaled, Xval_Scaled, Xtest_Scaled, Ytrain, Yval, Ytest)\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWLeux1ndb7l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1593344173895,"user_tz":-120,"elapsed":763,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"13997586-e96c-4207-b4a0-fb148403ee52"},"source":["(Xtrain, Xval, Xtest, ytrain, yval, ytest) = preprocesado(train_with_imgs, val_with_imgs, test_with_imgs)\n","\n","print(f'Dimensiones del dataset de training: {train_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de validación: {val_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de test: {test_imgs_loaded.shape}')\n","\n","print(f'Dimensiones del dataset de training: {Xtrain.shape}')\n","print(f'Dimensiones del dataset de validación: {Xval.shape}')\n","print(f'Dimensiones del dataset de test: {Xtest.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (7204, 224, 224, 3)\n","Dimensiones del dataset de validación: (1790, 224, 224, 3)\n","Dimensiones del dataset de test: (2277, 224, 224, 3)\n","Dimensiones del dataset de training: (7204, 47)\n","Dimensiones del dataset de validación: (1790, 47)\n","Dimensiones del dataset de test: (2277, 47)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5CGH2N4oH0vy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1593344175705,"user_tz":-120,"elapsed":1661,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"52e88dca-f1d7-410d-e6ac-82f0ffcbecb3"},"source":["#Redimensionamos las imágenes de entrada. Estoy teniendo problemas de RAM y no puedo ejecutarlo\n","#con 224x224 no puedo escalar /255. Con 112x112 no puedo ejecutar el modelo\n","#es necesario asumir esta pérdida de información\n","train_imgs_loaded = np.resize(train_imgs_loaded, (train_imgs_loaded.shape[0],64, 64, train_imgs_loaded.shape[3]))\n","val_imgs_loaded = np.resize(val_imgs_loaded, (val_imgs_loaded.shape[0],64, 64, val_imgs_loaded.shape[3]))\n","test_imgs_loaded = np.resize(test_imgs_loaded, (test_imgs_loaded.shape[0],64, 64, test_imgs_loaded.shape[3]))\n","\n","print(f'Dimensiones del dataset de training: {train_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de training: {val_imgs_loaded.shape}')\n","print(f'Dimensiones del dataset de training: {test_imgs_loaded.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dimensiones del dataset de training: (7204, 64, 64, 3)\n","Dimensiones del dataset de training: (1790, 64, 64, 3)\n","Dimensiones del dataset de training: (2277, 64, 64, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J7QHeDgeJzbC","colab_type":"code","colab":{}},"source":["#escalamos los datos de entrada. Lo hago en celdas separadas ya que hay algún problema de RAM\n","#se trata de imágenes así que no hace falta centrar, solo dividimos por el máximo. \n","# nos aseguramos de hacerlo como float para no perder la info de los decimales\n","\n","train_imgs_loaded = train_imgs_loaded.astype('float32') / 255.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lgn_PkHKiiV","colab_type":"code","colab":{}},"source":["val_imgs_loaded = val_imgs_loaded.astype('float32') / 255.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"La20DOHfLjYl","colab_type":"code","colab":{}},"source":["test_imgs_loaded = test_imgs_loaded.astype('float32') / 255."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvOWPiKEMIi5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593344186241,"user_tz":-120,"elapsed":537,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"fa9a8e1c-5142-4734-d79c-a0baae65eb5e"},"source":["from keras.utils import to_categorical\n","\n","# convertimos las etiquetas a one-hot encoding\n","num_classes = 3\n","Ytrain = to_categorical(ytrain, num_classes)\n","Yval = to_categorical(yval, num_classes)\n","Ytest = to_categorical(ytest, num_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yefHyTAEhshQ","colab_type":"text"},"source":["En este punto ya tenemos nuestros datos de entrada (imágenes por un lado y datos numéricos y categóricos por otro) preparados. Vamos a definir nuestros modelos de red neuronal para que traten cada uno de ese tipo de datos.\n","\n","Para ello nos basamos en los notebooks anteriores y elegimos directamente el modelo que mejor prestaciones nos dio.\n","\n","No obstante, eliminamos la última capa de cada uno de esos modelos. No queremos obtener el resultado final de la regresión por separado. Esas dos salidas las concatenamos, y ahora sí, las volvemos a introducir en un modelo que hace la regresión final."]},{"cell_type":"code","metadata":{"id":"TM90tSDZX4Ax","colab_type":"code","colab":{}},"source":["from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Flatten, Activation, Input\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, concatenate\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from keras.optimizers import Adam\n","from keras.constraints import max_norm\n","\n","\n","# Creamos la rama de la red convolucional para tratar las imágenes\n","def miCNN(width, height, depth):\n","  model = Sequential()\n","\n","  # Definimos una capa convolucional\n","  model.add(Conv2D(64, kernel_size=(5,5), input_shape=(64, 64, 3)))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.09569))\n","\n","  # Definimos una segunda capa convolucional\n","  model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.09569))\n","\n","  # Definimos una tercera capa convolucional\n","  model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Activation('relu'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.09569))\n","\n","  # Añadimos nuestro clasificador\n","  model.add(Flatten())\n","  model.add(Dense(256, activation='relu', kernel_constraint=max_norm(3.)))\n","  model.add(Dropout(0.09569))\n","  model.add(Dense(4, activation='relu'))\n","\n","  return model\n","\n","#creamos otra rama con una red neuronal para tratar los datos numéricos/categóricos\n","def miRed(dim):\n","  model = Sequential()\n","  model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n","  model.add(Dense(4, activation=\"relu\"))\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtXVqpBOYGWp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":457},"executionInfo":{"status":"ok","timestamp":1593344850170,"user_tz":-120,"elapsed":49874,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"f899f1ca-8f69-4b2d-b0b5-299dd83bdac4"},"source":["#cogemos las salidas de las dos ramas por separado y las concatenamos\n","#esto es la entrada combinada de nuestro modelo final que da como resultado la regresión\n","\n","dataBranch = miRed(Xtrain.shape[1])\n","imageBranch = miCNN(64,64,3)\n","combinedInput = concatenate([dataBranch.output, imageBranch.output])\n","\n","x = Dense(4, activation=\"relu\")(combinedInput)\n","x = Dense(num_classes, activation=\"softmax\")(x)\n","model = Model(inputs=[dataBranch.input, imageBranch.input], outputs=x)\n","\n","# Compilamos el modelo\n","opt = Adam(lr=1e-3, decay=1e-3 / 200)\n","model.compile(loss=\"categorical_crossentropy\", \n","              optimizer=opt,\n","\t\t\t\t\t\t\tmetrics=['accuracy'])\n","\n","# Entrenamos el modelo\n","print(\"[INFO] training model...\")\n","model.fit([Xtrain,train_imgs_loaded], Ytrain,\n","          batch_size=128,\n","\t\t\t\t\tshuffle=True,\n","\t\t\t\t\tepochs=10,\n","\t\t\t\t\tvalidation_data=([Xval, val_imgs_loaded], Yval))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] training model...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 7204 samples, validate on 1790 samples\n","Epoch 1/10\n","7204/7204 [==============================] - 11s 2ms/step - loss: 1.1086 - accuracy: 0.4213 - val_loss: 1.0717 - val_accuracy: 0.4950\n","Epoch 2/10\n","7204/7204 [==============================] - 4s 553us/step - loss: 1.0607 - accuracy: 0.4720 - val_loss: 1.0474 - val_accuracy: 0.4950\n","Epoch 3/10\n","7204/7204 [==============================] - 4s 555us/step - loss: 1.0381 - accuracy: 0.4629 - val_loss: 1.0259 - val_accuracy: 0.4480\n","Epoch 4/10\n","7204/7204 [==============================] - 4s 558us/step - loss: 1.0184 - accuracy: 0.4603 - val_loss: 1.0068 - val_accuracy: 0.4950\n","Epoch 5/10\n","7204/7204 [==============================] - 4s 556us/step - loss: 1.0011 - accuracy: 0.4713 - val_loss: 0.9901 - val_accuracy: 0.4950\n","Epoch 6/10\n","7204/7204 [==============================] - 4s 558us/step - loss: 0.9860 - accuracy: 0.4714 - val_loss: 0.9756 - val_accuracy: 0.4950\n","Epoch 7/10\n","7204/7204 [==============================] - 4s 560us/step - loss: 0.9730 - accuracy: 0.4649 - val_loss: 0.9630 - val_accuracy: 0.4950\n","Epoch 8/10\n","7204/7204 [==============================] - 4s 558us/step - loss: 0.9616 - accuracy: 0.4720 - val_loss: 0.9518 - val_accuracy: 0.4950\n","Epoch 9/10\n","7204/7204 [==============================] - 4s 561us/step - loss: 0.9516 - accuracy: 0.4715 - val_loss: 0.9421 - val_accuracy: 0.4950\n","Epoch 10/10\n","7204/7204 [==============================] - 4s 561us/step - loss: 0.9430 - accuracy: 0.4713 - val_loss: 0.9337 - val_accuracy: 0.4950\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f3555d59780>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"qwuo4QA0xM5o","colab_type":"text"},"source":["Por último evaluamos el modelo con el conjunto de test"]},{"cell_type":"code","metadata":{"id":"xnXJIhQWOKI8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1593345070800,"user_tz":-120,"elapsed":1149,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"85a26ee1-437a-4783-d4d4-ec84725c0663"},"source":["# Evaluamos el modelo\n","scores = model.evaluate([Xtest,test_imgs_loaded], Ytest)\n","\n","print('Loss: %.3f' % scores[0])\n","print('Accuracy: %.3f' % scores[1])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2277/2277 [==============================] - 1s 284us/step\n","Loss: 0.943\n","Accuracy: 0.477\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rQw0jasVxQW3","colab_type":"text"},"source":["seguimos teniendo el problema de la clasificación al 47% o 49%. En la siguiente celda se pueden ver las predicciones"]},{"cell_type":"code","metadata":{"id":"cDdrIzvQxDo5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":857},"executionInfo":{"status":"ok","timestamp":1593345227347,"user_tz":-120,"elapsed":1066,"user":{"displayName":"Marcos Gutiérrez","photoUrl":"","userId":"15097821019630428284"}},"outputId":"f0246ff8-4e55-49e5-a179-fefdac4dd90a"},"source":["pred = model.predict([Xval,val_imgs_loaded])\n","#imprimo  unas cuantas aleatorias, y siempre da lo mismo\n","pred[100:150,:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442],\n","       [0.41431552, 0.41137004, 0.17431442]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"nCOaNeYN1POc","colab_type":"text"},"source":["**CONCLUSIÓN FINAL**\n","\n","Viendo los resultados tenemos que elegir como el mejor de los 3 cláramente el modelo que trabaja con datos numéricos y categóricos. Nos dio una precisión del 85%.\n","\n","En el momento que hemos introducido las imágenes la precisión ha bajado mucho, casi al 50%.\n","\n","La principal razón puede ser el redimensionamiento que hemos tenido que hacer a la baja de las imágenes, ya que con el tamaño original no lo podía procesar."]},{"cell_type":"code","metadata":{"id":"gWgjBPKs32d0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}