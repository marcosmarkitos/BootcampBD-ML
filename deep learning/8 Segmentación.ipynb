{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8 SegmentacioÌn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NKaSROHz9cY7","colab_type":"code","colab":{}},"source":["# Fuente: \n","# https://aboveintelligent.com/image-segmentation-with-neural-net-d5094d571b1e\n","# https://github.com/mzaradzki/neuralnets/tree/master/vgg_segmentation_keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-d0kcmIxfDr9","colab_type":"code","colab":{}},"source":["!apt install python-pydot python-pydot-ng graphviz \n","!pip install pydot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4TO1wageaTE","colab_type":"code","colab":{}},"source":["import numpy as np\n","from keras.models import Sequential,Model\n","from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Deconvolution2D, Cropping2D\n","from keras.layers import Input, Add, Dropout, Permute, add\n","from keras.utils import plot_model\n","from scipy.io import loadmat\n","from scipy.misc import imread\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import copy\n","from scipy.misc import bytescale\n","from scipy.io import loadmat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EU4A4dqceaTK","colab_type":"code","colab":{}},"source":["# Function to create to a series of CONV layers followed by Max pooling layer\n","def convblock(cdim, nb, nfilt=3):\n","\tL = []\n","\n","\tfor k in range(1, nfilt + 1):\n","\t\tconvname = 'conv' + str(nb) + '_' + str(k)\n","\t\tL.append(Convolution2D(cdim, kernel_size=(3, 3), padding='same', activation='relu', name=convname))\n","\n","\tL.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n","\n","\treturn L"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7K7Dydt8eaTM","colab_type":"code","colab":{}},"source":["#Helper function to create Sequential part of the Architecture\n","def fcn32_blank(image_size=512):\n","\t\n","  mdl = Sequential()\n","\n","  # First layer is a dummy-permutation = Identity to specify input shape\n","  mdl.add(Permute((1, 2, 3), input_shape=(image_size, image_size, 3)))  # WARNING : axis 0 is the sample dim\n","\n","  for l in convblock(64, 1, nfilt=2):\n","    mdl.add(l)\n","\n","  for l in convblock(128, 2, nfilt=2):\n","    mdl.add(l)\n","\n","  for l in convblock(256, 3, nfilt=3):\n","    mdl.add(l)\n","\n","  for l in convblock(512, 4, nfilt=3):\n","    mdl.add(l)\n","\n","  for l in convblock(512, 5, nfilt=3):\n","    mdl.add(l)\n","\n","  mdl.add(Convolution2D(4096, kernel_size=(7, 7), padding='same', activation='relu', name='fc6'))  # WARNING border\n","  mdl.add(Convolution2D(4096, kernel_size=(1, 1), padding='same', activation='relu', name='fc7'))  # WARNING border\n","  \n","  # WARNING : model decapitation i.e. remove the classifier step of VGG16 (usually named fc8)\n","\n","  mdl.add(Convolution2D(21, kernel_size=(1, 1), padding='same', activation='relu', name='score_fr'))\n","\n","  convsize = mdl.layers[-1].output_shape[2]\n","  deconv_output_size = (convsize - 1) * 2 + 4  # INFO: =34 when images are 512x512\n","  mdl.add(Deconvolution2D(21, kernel_size=(4, 4), strides=(2, 2), padding='valid', activation=None, name='score2'))\n","\n","  extra_margin = deconv_output_size - convsize * 2  # INFO: =2 when images are 512x512\n","  assert (extra_margin > 0)\n","  assert (extra_margin % 2 == 0)\n","  # INFO : cropping as deconv gained pixels\n","  c = ((0, extra_margin), (0, extra_margin))\n","  mdl.add(Cropping2D(cropping=((extra_margin/2, extra_margin/2), (extra_margin/2, extra_margin/2))) ) # INFO : cropping as deconv gained pixels\n","  \n","  return mdl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtEye6sFeaTQ","colab_type":"code","colab":{}},"source":["def fcn_32s_to_16s(fcn32model=None):\n","  if fcn32model is None:\n","    fcn32model = fcn32_blank()\n","\n","  fcn32shape = fcn32model.layers[-1].output_shape\n","  assert (len(fcn32shape) == 4)\n","  assert (fcn32shape[0] is None)  # batch axis\n","  assert (fcn32shape[3] == 21)  # number of filters\n","  assert (fcn32shape[1] == fcn32shape[2])  # must be square\n","\n","  fcn32size = fcn32shape[1]  # INFO: =32 when images are 512x512\n","\n","  if fcn32size != 32:\n","    print('WARNING : handling of image size different from 512x512 has not been tested')\n","\n","  sp4 = Convolution2D(21, kernel_size=(1, 1), padding='same', activation=None, name='score_pool4')\n","\n","  # INFO : to replicate MatConvNet.DAGN.Sum layer see documentation at :\n","  # https://keras.io/getting-started/sequential-model-guide/\n","  summed = add(inputs=[sp4(fcn32model.layers[14].output), fcn32model.layers[-1].output])\n","\n","  # INFO :\n","  # final 16x16 upsampling of \"summed\" using deconv layer upsample_new (32, 32, 21, 21)\n","  # deconv setting is valid if (528-32)/16 + 1 = deconv_input_dim (= fcn32size)\n","  deconv_output_size = (fcn32size - 1) * 16 + 32  # INFO: =528 when images are 512x512\n","  upnew = Deconvolution2D(21, kernel_size=(32, 32),\n","              padding='valid',  # WARNING : valid, same or full ?\n","              strides=(16, 16),\n","              activation=None,\n","              name='upsample_new')\n","\n","  extra_margin = deconv_output_size - fcn32size * 16  # INFO: =16 when images are 512x512\n","  assert (extra_margin > 0)\n","  assert (extra_margin % 2 == 0)\n","  # INFO : cropping as deconv gained pixels\n","  crop_margin = Cropping2D(cropping=((extra_margin/2, extra_margin/2), (extra_margin/2, extra_margin/2))) # INFO : cropping as deconv gained pixels\n","\n","  return Model(fcn32model.input, crop_margin(upnew(summed)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SH-BUSc32M1T","colab_type":"code","colab":{}},"source":["def prediction(kmodel, crpimg, transform=False):\n","\t# INFO : crpimg should be a cropped image of the right dimension\n","\n","\timarr = np.array(crpimg).astype(np.float32)\n","\n","\tif transform:\n","\t\timarr[:, :, 0] -= 129.1863\n","\t\timarr[:, :, 1] -= 104.7624\n","\t\timarr[:, :, 2] -= 93.5940\n","\t\t#\n","\t\t# WARNING : in this script (https://github.com/rcmalli/keras-vggface) colours are switched\n","\t\taux = copy.copy(imarr)\n","\t\timarr[:, :, 0] = aux[:, :, 2]\n","\t\timarr[:, :, 2] = aux[:, :, 0]\n","\n","\timarr = np.expand_dims(imarr, axis=0)\n","\n","\treturn kmodel.predict(imarr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6QFs6aK4gFl","colab_type":"code","colab":{}},"source":["fcn32model = fcn32_blank()\n","print(fcn32model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXgArME34jmd","colab_type":"code","colab":{}},"source":["fcn16model = fcn_32s_to_16s(fcn32model)\n","print(fcn16model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-F5O6p4PeaTb","colab_type":"code","colab":{}},"source":["plot_model(fcn16model,'FCN-16_withshape.png',show_shapes=True)\n","plt.figure(figsize=(150,40))\n","plt.imshow(imread('FCN-16_withshape.png'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXmHLxfufeMd","colab_type":"code","colab":{}},"source":["!wget http://www.vlfeat.org/matconvnet/models/pascal-fcn16s-dag.mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8FYOkw7eaTj","colab_type":"code","colab":{}},"source":["data = loadmat('pascal-fcn16s-dag.mat', matlab_compatible=False, struct_as_record=False)\n","layers = data['layers']\n","params = data['params']\n","description = data['meta'][0,0].classes[0,0].description"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Nr9B6baeaTm","colab_type":"code","colab":{}},"source":["print(data.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ng-9V7-eaTp","colab_type":"code","colab":{}},"source":["print(type(layers))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWLIDk-8eaTt","colab_type":"code","colab":{}},"source":["print(layers.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nv8GUWW2eaTx","colab_type":"code","colab":{}},"source":["class2index = {}\n","for i, clname in enumerate(description[0,:]):\n","    class2index[str(clname[0])] = i\n","    \n","print(sorted(class2index.keys()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FdOcyZzeaT0","colab_type":"code","colab":{}},"source":["for i in range(0, params.shape[1]-1, 2):\n","    print(i,\n","          str(params[0,i].name[0]), params[0,i].value.shape,\n","          str(params[0,i+1].name[0]), params[0,i+1].value.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EG5d3WL4eaT3","colab_type":"code","colab":{}},"source":["for i in range(layers.shape[1]):\n","    print(i,\n","          str(layers[0,i].name[0]), str(layers[0,i].type[0]),\n","          [str(n[0]) for n in layers[0,i].inputs[0,:]],\n","          [str(n[0]) for n in layers[0,i].outputs[0,:]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKLGnH1heaT6","colab_type":"code","colab":{}},"source":["def copy_mat_to_keras(kmodel):\n","    \n","    kerasnames = [lr.name for lr in kmodel.layers]\n","\n","    prmt = (0, 1, 2, 3) # WARNING : important setting as 2 of the 4 axis have same size dimension\n","    \n","    for i in range(0, params.shape[1]-1, 2):\n","        matname = '_'.join(params[0,i].name[0].split('_')[0:-1])\n","        if matname in kerasnames:\n","            kindex = kerasnames.index(matname)\n","            print('found : ', (str(matname), kindex))\n","            l_weights = params[0,i].value\n","            l_bias = params[0,i+1].value\n","            f_l_weights = l_weights.transpose(prmt)\n","            if False: # WARNING : this depends on \"image_data_format\":\"channels_last\" in keras.json file\n","                f_l_weights = np.flip(f_l_weights, 0)\n","                f_l_weights = np.flip(f_l_weights, 1)\n","            print(f_l_weights.shape, kmodel.layers[kindex].get_weights()[0].shape)\n","            assert (f_l_weights.shape == kmodel.layers[kindex].get_weights()[0].shape)\n","            assert (l_bias.shape[1] == 1)\n","            assert (l_bias[:,0].shape == kmodel.layers[kindex].get_weights()[1].shape)\n","            assert (len(kmodel.layers[kindex].get_weights()) == 2)\n","            kmodel.layers[kindex].set_weights([f_l_weights, l_bias[:,0]])\n","        else:\n","            print('not found : ', str(matname))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5erUv5jYeaT9","colab_type":"code","colab":{}},"source":["copy_mat_to_keras(fcn16model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKJ_oX7X1-2U","colab_type":"code","colab":{}},"source":["!wget http://www.robots.ox.ac.uk/~szheng/crfasrnndemo/static/rgb.jpg\n","image_size = 512\n","im = Image.open('rgb.jpg') # http://www.robots.ox.ac.uk/~szheng/crfasrnndemo/static/rgb.jpg\n","im = im.crop((0,0,319,319)) # WARNING : manual square cropping\n","im = im.resize((image_size,image_size))\n","plt.imshow(np.asarray(im))\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7pm5KCy2scL","colab_type":"code","colab":{}},"source":["crpim = im # WARNING : we deal with cropping in a latter section, this image is already fit\n","preds = prediction(fcn16model, crpim, transform=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGNJ8OZ63BG9","colab_type":"code","colab":{}},"source":["preds.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvnJ2c6Q2-q7","colab_type":"code","colab":{}},"source":["imclass = np.argmax(preds, axis=3)[0,:,:]\n","\n","\n","plt.figure(figsize = (15, 7))\n","plt.subplot(1,3,1)\n","plt.imshow(crpim)\n","plt.axis('off')\n","\n","plt.subplot(1,3,2)\n","plt.imshow(imclass, cmap='jet')\n","plt.axis('off')\n","\n","plt.subplot(1,3,3)\n","plt.imshow(crpim)\n","plt.axis('off')\n","\n","masked_imclass = np.ma.masked_where(imclass == 0, imclass)\n","plt.imshow(masked_imclass, alpha=0.5, cmap='jet')\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiRPGaH43YCz","colab_type":"code","colab":{}},"source":["# List of dominant classes found in the image# List o \n","for c in np.unique(imclass):\n","    print c, str(description[0,c][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKW2qS6A4Lln","colab_type":"code","colab":{}},"source":["bspreds = bytescale(preds, low=0, high=255)\n","\n","plt.figure(figsize = (15, 7))\n","\n","plt.subplot(2,3,1)\n","plt.imshow(np.asarray(crpim))\n","plt.axis('off')\n","\n","plt.subplot(2,3,3+1)\n","plt.imshow(bspreds[0,:,:,class2index['background']], cmap='seismic')\n","plt.axis('off')\n","\n","plt.subplot(2,3,3+2)\n","plt.imshow(bspreds[0,:,:,class2index['person']], cmap='seismic')\n","plt.axis('off')\n","\n","plt.subplot(2,3,3+3)\n","plt.imshow(bspreds[0,:,:,class2index['bicycle']], cmap='seismic')\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aD3DHUv6hAf","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}