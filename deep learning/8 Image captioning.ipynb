{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8 Image captioning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NICAGePtXD5R","colab_type":"text"},"source":["Vamos a usar el dataset de Flickr8K que se compone de 8000 fotografías con sus descripciones.\n","\n","Lo primero que haremos es descargarnos el dataset:"]},{"cell_type":"code","metadata":{"id":"E0Drjs0VYIyQ","colab_type":"code","colab":{}},"source":["!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n","!unzip -o Flickr8k_text.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aKFjz_-7xM1n","colab_type":"code","colab":{}},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1brhYXSDYT5q","colab_type":"text"},"source":["Ahora, vamos a preparar nuestros datos. Para ello, vamos a crear unas cuantas funciones axiliares:"]},{"cell_type":"code","metadata":{"id":"xHWWg4FrXIVP","colab_type":"code","colab":{}},"source":["def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n"," \n","filename = 'Flickr8k.token.txt'\n","# load descriptions\n","doc = load_doc(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcY3MeoWYm3l","colab_type":"code","colab":{}},"source":["len(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Twpvq7OVYtMj","colab_type":"code","colab":{}},"source":["doc[:255]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7IQBgk6Yjtz","colab_type":"code","colab":{}},"source":["# extract descriptions for images\n","def load_descriptions(doc):\n","\tmapping = dict()\n","\t# process lines\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\tif len(line) < 2:\n","\t\t\tcontinue\n","\t\t# take the first token as the image id, the rest as the description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# remove filename from image id\n","\t\timage_id = image_id.split('.')[0]\n","\t\t# convert description tokens back to string\n","\t\timage_desc = ' '.join(image_desc)\n","\t\t# store the first description for each image\n","\t\tif image_id not in mapping:\n","\t\t\tmapping[image_id] = image_desc\n","\treturn mapping\n"," \n","# parse descriptions\n","descriptions = load_descriptions(doc)\n","print('Loaded: %d ' % len(descriptions))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_FOhTNZNYDA8","colab_type":"text"},"source":["Ahora, vamos a limpiar las descripciones. Necesitamos convertir todas las palabras a minúsculas, quitar la puntuación y quitar también las palabras que son de 1 solo caracter: por ejemplo, \"a\". Tened en cuenta que el dataset está en inglés.\n","\n","Vamos allá:"]},{"cell_type":"code","metadata":{"id":"nuDCOE67ZBzv","colab_type":"code","colab":{}},"source":["import string\n","\n","def clean_descriptions(descriptions):\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor key, desc in descriptions.items():\n","\t\t# tokenize\n","\t\tdesc = desc.split()\n","\t\t# convert to lower case\n","\t\tdesc = [word.lower() for word in desc]\n","\t\t# remove punctuation from each token\n","\t\tdesc = [w.translate(table) for w in desc]\n","\t\t# remove hanging 's' and 'a'\n","\t\tdesc = [word for word in desc if len(word)>1]\n","\t\t# store as string\n","\t\tdescriptions[key] =  ' '.join(desc)\n","\n","# clean descriptions\n","clean_descriptions(descriptions)\n","# summarize vocabulary\n","all_tokens = ' '.join(descriptions.values()).split()\n","vocabulary = set(all_tokens)\n","print('Vocabulary Size: %d' % len(vocabulary))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sr-OvPt3ZMq_","colab_type":"text"},"source":["Y nos guardamos el dataset arreglado para poder usarlo más cómodamente en otras ocasiones:"]},{"cell_type":"code","metadata":{"id":"1_DUKxEAZQzF","colab_type":"code","colab":{}},"source":["# save descriptions to file, one per line\n","def save_doc(descriptions, filename):\n","\tlines = list()\n","\tfor key, desc in descriptions.items():\n","\t\tlines.append(key + ' ' + desc)\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n","\n","# save descriptions\n","save_doc(descriptions, 'descriptions.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lZLUakRZUW1","colab_type":"code","colab":{}},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PtFEyRQoZbeI","colab_type":"text"},"source":["Y si miramos nuestro dataset ahora..."]},{"cell_type":"code","metadata":{"id":"fVljF0AlZdTr","colab_type":"code","colab":{}},"source":["type(descriptions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sh_1Z2sMZo2n","colab_type":"code","colab":{}},"source":["descriptions.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOSISopKZiXE","colab_type":"code","colab":{}},"source":["descriptions.values()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bw4CEE4ZzOr","colab_type":"text"},"source":["Vale, pues hasta aquí la preparación del dataset de texto! Ahora vamos a preparar las fotos. \n","\n","Lo que vamos a hacer es usar la VGG16 para extraer las características de las imagenes, y nos guardaremos estas características. Así que vamos a cargar el modelo sin el top_model, a predecir lo que nos da cada una de nuestras imágenes, y a guardar sus características en un archivo que luego usaremos para alimentar nuestra LSTM. Podríamos hacerlo \"online\", es decir, predecir y después metérselo a la LSTM sin guardarlo en disco, pero para que nuestro modelo sea más rápido a la hora de entrenar, vamos a primero, predecir y guardar las características, y luego entrenar la LSTM.\n","\n","Para ello, vamos a bajarnos el dataset de imágenes:"]},{"cell_type":"code","metadata":{"id":"MisN0evdcR5H","colab_type":"code","colab":{}},"source":["rm -rf Flickr8k_Dataset.zip*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhsoPPBRcVcN","colab_type":"code","colab":{}},"source":["ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhedzOfZauZl","colab_type":"code","colab":{}},"source":["!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n","!unzip -q Flickr8k_Dataset.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcOSCM1wbMO3","colab_type":"code","colab":{}},"source":["# comprobamos si se han descomprimido\n","!ls -ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJj3yUH8B8xL","colab_type":"code","colab":{}},"source":["!pip install tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VD9CUW6GQ1R","colab_type":"code","colab":{}},"source":["from os import listdir\n","from pickle import load, dump\n","from tqdm import tqdm\n","from numpy import array\n","from numpy import argmax\n","from pandas import DataFrame\n","from nltk.translate.bleu_score import corpus_bleu\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.applications.vgg16 import VGG16\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.models import Model\n","from keras.layers import Input, Dense, Flatten, LSTM, RepeatVector, TimeDistributed, Embedding\n","from keras.layers.merge import concatenate\n","from keras.layers.pooling import GlobalMaxPooling2D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BUaXot9akwi","colab_type":"code","colab":{}},"source":["# extract features from each photo in the directory\n","def extract_features(directory):\n","  # load the model\n","  in_layer = Input(shape=(224, 224, 3))\n","  model = VGG16(include_top=False, input_tensor=in_layer)\n","  print(model.summary())\n","  # extract features from each photo\n","  features = dict()\n","\n","  files_in_directory = listdir(directory)\n","  n_images = len(files_in_directory)\n","  for i, name in tqdm(enumerate(files_in_directory)):\n","    # load an image from file\n","    filename = directory + '/' + name\n","    image = load_img(filename, target_size=(224, 224))\n","    # convert the image pixels to a numpy array\n","    image = img_to_array(image)\n","    # reshape data for the model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # prepare the image for the VGG model\n","    image = preprocess_input(image)\n","    # get features\n","    feature = model.predict(image, verbose=0)\n","    # get image id\n","    image_id = name.split('.')[0]\n","    # store feature\n","    features[image_id] = feature\n","    # print('{} / {} > {}'.format(i, n_images, name))\n","  return features\n","\n","# extract features from all images\n","directory = 'Flicker8k_Dataset'\n","features = extract_features(directory)\n","print('Extracted Features: %d' % len(features))\n","# save to file\n","dump(features, open('features.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNCambQEcgSr","colab_type":"code","colab":{}},"source":["from pickle import load\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# split a dataset into train/test elements\n","def train_test_split(dataset):\n","\t# order keys so the split is consistent\n","\tordered = sorted(dataset)\n","\t# return split dataset as two new sets\n","\treturn set(ordered[:100]), set(ordered[100:200])\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# store\n","\t\t\tdescriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\treturn descriptions\n","\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n","\n","# load dev set\n","filename = 'Flickr_8k.devImages.txt'\n","dataset = load_set(filename)\n","print('Dataset: %d' % len(dataset))\n","# train-test split\n","train, test = train_test_split(dataset)\n","print('Train=%d, Test=%d' % (len(train), len(test)))\n","# descriptions\n","train_descriptions = load_clean_descriptions('descriptions.txt', train)\n","test_descriptions = load_clean_descriptions('descriptions.txt', test)\n","print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n","# photo features\n","train_features = load_photo_features('features.pkl', train)\n","test_features = load_photo_features('features.pkl', test)\n","print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_UQ4A32hyzz","colab_type":"code","colab":{}},"source":["# Ahora codificamos nuestras descripciones a números\n","from keras.preprocessing.text import Tokenizer\n","\n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = list(descriptions.values())\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# prepare tokenizer\n","tokenizer = create_tokenizer(descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Hofs9qBiGS6","colab_type":"text"},"source":["For example, the input sequence “little girl running in field” would be split into 6 input-output pairs to train the model:\n","\n","\n","X1,\t\tX2 (text sequence), \t\t\t\t\t\ty (word)\n","\n","photo\tstartseq, \t\t\t\t\t\t\t\t\tlittle\n","\n","photo\tstartseq, little,\t\t\t\t\t\t\tgirl\n","\n","photo\tstartseq, little, girl, \t\t\t\t\trunning\n","\n","photo\tstartseq, little, girl, running, \t\t\tin\n","\n","photo\tstartseq, little, girl, running, in, \t\tfield\n","\n","photo\tstartseq, little, girl, running, in, field, endseq\n"]},{"cell_type":"code","metadata":{"id":"10UJu30Gh-jF","colab_type":"code","colab":{}},"source":["# Y creamos las secuencias:\n","\n","# create sequences of images, input sequences and output words for an image\n","def create_sequences(tokenizer, desc, image, max_length):\n","\tXimages, XSeq, y = list(), list(),list()\n","\tvocab_size = len(tokenizer.word_index) + 1\n","\t# integer encode the description\n","\tseq = tokenizer.texts_to_sequences([desc])[0]\n","\t# split one sequence into multiple X,y pairs\n","\tfor i in range(1, len(seq)):\n","\t\t# select\n","\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t# pad input sequence\n","\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\t\t# encode output sequence\n","\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\t\t# store\n","\t\tXimages.append(image)\n","\t\tXSeq.append(in_seq)\n","\t\ty.append(out_seq)\n","\t# Ximages, XSeq, y = array(Ximages), array(XSeq), array(y)\n","\treturn [Ximages, XSeq, y]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7oIGJH3iXTQ","colab_type":"code","colab":{}},"source":["# define the captioning model\n","def define_model(vocab_size, max_length):\n","\t# feature extractor (encoder)\n","\tinputs1 = Input(shape=(7, 7, 512))\n","\tfe1 = GlobalMaxPooling2D()(inputs1)\n","\tfe2 = Dense(128, activation='relu')(fe1)\n","\tfe3 = RepeatVector(max_length)(fe2)\n","\t# embedding\n","\tinputs2 = Input(shape=(max_length,))\n","\temb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n","\temb3 = LSTM(256, return_sequences=True)(emb2)\n","\temb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n","\t# merge inputs\n","\tmerged = concatenate([fe3, emb4])\n","\t# language model (decoder)\n","\tlm2 = LSTM(500)(merged)\n","\tlm3 = Dense(500, activation='relu')(lm2)\n","\toutputs = Dense(vocab_size, activation='softmax')(lm3)\n","\t# tie it together [image, seq] [word]\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\tprint(model.summary())\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Up4RUQmCifwW","colab_type":"code","colab":{}},"source":["# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(descriptions, features, tokenizer, max_length, n_step):\n","\t# loop until we finish training\n","\twhile 1:\n","\t\t# loop over photo identifiers in the dataset\n","\t\tkeys = list(descriptions.keys())\n","\t\tfor i in range(0, len(keys), n_step):\n","\t\t\tXimages, XSeq, y = list(), list(),list()\n","\t\t\tfor j in range(i, min(len(keys), i+n_step)):\n","\t\t\t\timage_id = keys[j]\n","\t\t\t\t# retrieve photo feature input\n","\t\t\t\timage = features[image_id][0]\n","\t\t\t\t# retrieve text input\n","\t\t\t\tdesc = descriptions[image_id]\n","\t\t\t\t# generate input-output pairs\n","\t\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n","\t\t\t\tfor k in range(len(in_img)):\n","\t\t\t\t\tXimages.append(in_img[k])\n","\t\t\t\t\tXSeq.append(in_seq[k])\n","\t\t\t\t\ty.append(out_word[k])\n","\t\t\t# yield this batch of samples to the model\n","\t\t\tyield [[array(Ximages), array(XSeq)], array(y)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDe_Ew7PijGN","colab_type":"code","colab":{}},"source":["# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n"," \n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text\n"," \n","# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\tactual.append([desc.split()])\n","\t\tpredicted.append(yhat.split())\n","\t# calculate BLEU score\n","\tbleu = corpus_bleu(actual, predicted)\n","\treturn bleu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBOj70V1KVrp","colab_type":"text"},"source":["Como hemos entrenado con datasets muy pequeños, nuestro dataset tiene gran variabilidad. Con lo cual, vamos a ejecutar 3 rondas de experimentos (n_repeats) y a promediar las métricas para poder dar una métrica más fiable."]},{"cell_type":"code","metadata":{"id":"bsxeXuYsGcZ9","colab_type":"code","colab":{}},"source":["# load dev set\n","filename = 'Flickr_8k.devImages.txt'\n","dataset = load_set(filename)\n","print('Dataset: %d' % len(dataset))\n","# train-test split\n","train, test = train_test_split(dataset)\n","# descriptions\n","train_descriptions = load_clean_descriptions('descriptions.txt', train)\n","test_descriptions = load_clean_descriptions('descriptions.txt', test)\n","print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n","# photo features\n","train_features = load_photo_features('features.pkl', train)\n","test_features = load_photo_features('features.pkl', test)\n","print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max(len(s.split()) for s in list(train_descriptions.values()))\n","print('Description Length: %d' % max_length)\n"," \n","# define experiment\n","model_name = 'baseline1'\n","verbose = 1\n","n_epochs = 50\n","n_photos_per_update = 2\n","n_batches_per_epoch = int(len(train) / n_photos_per_update)\n","n_repeats = 3\n"," \n","# run experiment\n","train_results, test_results = list(), list()\n","for i in range(n_repeats):\n","\t# define the model\n","\tmodel = define_model(vocab_size, max_length)\n","\t# fit model\n","\tmodel.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n","\t# evaluate model on training data\n","\ttrain_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n","\ttest_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n","\t# store\n","\ttrain_results.append(train_score)\n","\ttest_results.append(test_score)\n","\tprint('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n","# save results to file\n","df = DataFrame()\n","df['train'] = train_results\n","df['test'] = test_results\n","print(df.describe())\n","df.to_csv(model_name+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxW6Vv53KjeX","colab_type":"text"},"source":["Fijaos que nos da unas pérdidas medias son 0.02 en train y 0.05 en test. La verdad es que esto no nos dice mucho, así que vamos a probar a ejecutarlo viendo realmente lo que predice nuestra red LSTM:"]},{"cell_type":"code","metadata":{"id":"eOo398CQklmM","colab_type":"code","colab":{}},"source":["# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\tactual.append([desc.split()])\n","\t\tpredicted.append(yhat.split())\n","\t\tprint('Actual:    %s' % desc)\n","\t\tprint('Predicted: %s' % yhat)\n","\t\tif len(actual) >= 5:\n","\t\t\tbreak\n","\t# calculate BLEU score\n","\tbleu = corpus_bleu(actual, predicted)\n","\treturn bleu\n","\n","# define experiment\n","model_name = 'baseline1'\n","verbose = 2\n","n_epochs = 50\n","n_photos_per_update = 2\n","n_batches_per_epoch = int(len(train) / n_photos_per_update)\n","n_repeats = 3\n"," \n","# run experiment\n","train_results, test_results = list(), list()\n","for i in range(n_repeats):\n","\t# define the model\n","\tmodel = define_model(vocab_size, max_length)\n","\t# fit model\n","\tmodel.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n","\t# evaluate model on training data\n","\ttrain_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n","\ttest_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n","\t# store\n","\ttrain_results.append(train_score)\n","\ttest_results.append(test_score)\n","\tprint('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n","# save results to file\n","df = DataFrame()\n","df['train'] = train_results\n","df['test'] = test_results\n","print(df.describe())\n","df.to_csv(model_name+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYOT_ooJK4Ox","colab_type":"text"},"source":["Según podéis ver, las descripciones que damos de nuestras imágenes no son las más precisas del mundo. Esto es porque este ejemplo usa muy pocos datos, en la FUENTE que tenéis más abajo, que es de donde he extraído este ejemplo, podéis ver cómo mejoran lo que hemos hecho aquí.\n","\n","FUENTE: https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/"]},{"cell_type":"markdown","metadata":{"id":"Xkzrk1Fok2jN","colab_type":"text"},"source":["Os dejo también varios enlaces que podéis consultar para conseguir hacer algo mejor todavía:\n","\n","* https://machinelearningmastery.com/how-to-caption-photos-with-deep-learning/\n","* https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n","* https://daniel.lasiman.com/post/image-captioning/\n","* https://www.oreilly.com/learning/caption-this-with-tensorflow\n","* https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2\n","* https://deeplearningmania.quora.com/Keras-deep-learning-for-image-caption-retrieval\n"]},{"cell_type":"code","metadata":{"id":"hwK1kAtIk3RW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}