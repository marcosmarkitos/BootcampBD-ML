{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7 Redes recurrentes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Is_eITHCjKjJ","colab_type":"text"},"source":["# 7. Redes recurrentes\n","\n","¡Bienvenidos a la séptima sesión! Ya sabemos lo que son las redes neuronales, las redes convolucionales y cómo optimizar sus parámetros al máximo usando diferentes métodos, desde fuerza bruta hasta algoritmos genéticos. Así que hoy vamos a empezar con una arquitectura de red totalmente diferente, las **redes recurrentes**.\n","\n","Para ello, hoy veremos:\n","\n","**Redes recurrentes**\n","* Introducción\n","* Long Short-Term Memory (LSTM) networks\n","* Aplicación a series temporales\n"]},{"cell_type":"markdown","metadata":{"id":"MK8MAsqOtomN","colab_type":"text"},"source":["## 7.1 Introducción\n","\n","Bueno, qué son las redes recurrentes y para qué se usan?\n","\n","Lo primero de todo vamos a pensar en una cosa: las CNNs no tienen ningún tipo de memoria, verdad? Es decir, cuando nosotros estamos en la calle utilizamos ese conocimiento para saber que lo más probable es que veamos coches, autobuses y otras personas. Es poco probable que veamos sofás o duchas, verdad? Pues las RNN superan esta limitación que tienen las CNNs de \"empezar a pensar de cero\" cada vez, porque tienen cierta **memoria**.\n","\n","### Ejemplo simple\n","\n","Vamos a verlo siguiendo con un ejemplo: el del perfecto compañero de piso. Imagináos que vivimos en un piso y que nuestro compañero de piso es estupendo, porque cada día cocina una cosa distinta según el tiempo que hace, soleado o lluvioso.\n","\n","<img src=\"https://image.ibb.co/gSmT3J/nnintro_perfect_roomate.png\" alt=\"nnintro_perfect_roomate\" border=\"0\" height=\"200\">\n","\n","<img src=\"https://image.ibb.co/nCSCVy/nnintro_perfect_roomate_weather.png\" alt=\"nnintro_perfect_roomate_weather\" border=\"0\" height=\"200\">\n","\n","Si hace sol, tarta de manzana, si llueve, hamburguesa. La forma de codificar las comidas y el tiempo es la siguiente:\n","\n","<img src=\"https://image.ibb.co/h32Pcd/nnintro_perfect_roomate_vectors.png\" alt=\"nnintro_perfect_roomate_vectors\" border=\"0\" height=\"200\">\n","\n","Y esto es lo que hace nuestra red neuronal:\n","\n","<img src=\"https://image.ibb.co/exmZtJ/nnintro_example_gif.gif\" alt=\"nnintro_example_gif\" border=\"0\" height=\"200\">\n","\n","Acordaos que lo que aprenden las redes neuronales son unos pesos que pueden expresarse como una matriz. Aquí tenemos la nuestra:\n","\n","* Si el día es soleado\n","\n","<img src=\"https://image.ibb.co/d98fHd/nnintro_perfect_roomate_applepie.png\" alt=\"nnintro_perfect_roomate_applepie\" border=\"0\" height=\"150\">\n","\n","* Si el día está nublado\n","\n","<img src=\"https://image.ibb.co/n2JQiJ/nnintro_perfect_roomate_burguer.png\" alt=\"nnintro_perfect_roomate_burguer\" border=\"0\" height=\"150\">\n","\n","Y nuestra matriz de pesos vista en forma de grafo:\n","\n","<img src=\"https://image.ibb.co/m9LD3J/nnintro_perfect_roomate_matrix_graph.png\" alt=\"nnintro_perfect_roomate_matrix_graph\" border=\"0\" height=\"200\">\n","\n","\n","Vale, hasta aquí nada nuevo, verdad? Pues vamos a ver qué es lo que añaden las **redes recurrentes** siguiendo con este mismo ejemplo.\n","\n","### Redes recurrentes\n","\n","Pongamos que ahora nuestro querido compañero de piso no solamente basa la decisión de qué cocina en el tiempo, si no que ahora simplemente se fija en lo que cocinó ayer.\n","\n","<img src=\"https://image.ibb.co/fVD3nd/rnnintro_perfect_roomate_food_cycle.png\" alt=\"rnnintro_perfect_roomate_food_cycle\" border=\"0\" height=\"75\">\n","\n","Bien amigos, pues la red encargada de conseguir predecir lo que cocinará vuestro querido *roommate* mañana en función de lo que cocinó hoy es una:\n","\n","<img src=\"https://image.ibb.co/c0GpSd/rnnintro_perfect_roomate_food_cycle_gif.gif\" alt=\"rnnintro_perfect_roomate_food_cycle_gif\" border=\"0\" height=\"200\">\n","\n","Que se puede expresar como la matriz que podéis ver a continuación, y funciona así:\n","\n","<img src=\"https://image.ibb.co/k0jOLy/nnintro_rnn_gif.gif\" alt=\"nnintro_rnn_gif\" border=\"0\" height=\"250\">\n","\n","Que en forma de grafo, se puede expresar así:\n","\n","<img src=\"https://image.ibb.co/fsTHfy/nnintro_rnn_graph_gif.gif\" alt=\"nnintro_rnn_graph_gif\" border=\"0\" height=\"250\">\n","\n","Así que realmente, lo que tenemos al final es esto:\n","\n","<img src=\"https://image.ibb.co/iL2ztJ/nnintro_rnn_graph.png\" alt=\"nnintro_rnn_graph\" border=\"0\" height=\"200\">"]},{"cell_type":"markdown","metadata":{"id":"P5d-5btpo6yS","colab_type":"text"},"source":["### Mas complicado!"]},{"cell_type":"markdown","metadata":{"id":"VFsPRqJ8mJS3","colab_type":"text"},"source":["Genial!! Pero vamos a complicarlo un poco más! Imagináos ahora que vuestro compañero decide lo que cocina en función de lo que cocinó ayer y del tiempo que hace. En concreto, si sale el día soleado, se pasa el día en la terracita con una buena birra en la mano, con lo cual no cocina, así que comemos lo mismo de ayer. Pero si sale lluvioso, se queda en casa y si que cocina. Sería algo así:\n","\n","<img src=\"https://image.ibb.co/bUJ3nd/rnn_food_weather.png\" alt=\"rnn_food_weather\" border=\"0\" height=\"150\">\n","\n","Con lo que tenemos una parte que nos modela lo que nos tocaría comer en función de lo que comimos ayer:\n","\n","<img src=\"https://image.ibb.co/gL77DJ/nnintro_rnn_food_weather_gif.gif\" alt=\"nnintro_rnn_food_weather_gif\" border=\"0\" height=\"250\">\n","\n","Y otra que nos dice si nos cocinan o si se va al bar:\n","\n","<img src=\"https://image.ibb.co/jC6g7d/rnn_food_weather_gif.gif\" alt=\"rnn_food_weather_gif\" border=\"0\" height=\"250\">\n","\n","Por lo que al final, tenemos una combinación de las dos:\n","\n","<img src=\"https://image.ibb.co/cdcA0y/rnn_food_weather_matrices.png\" alt=\"rnn_food_weather_matrices\" border=\"0\" height=\"250\">\n","\n","Y las operaciones Add y Merge son las siguientes:\n","\n","<img src=\"https://image.ibb.co/ciutnd/rnn_food_weather_add.png\" alt=\"rnn_food_weather_add\" border=\"0\" height=\"200\">\n","\n","<img src=\"https://image.ibb.co/fnKhfy/rnn_food_weather_merge.png\" alt=\"rnn_food_weather_merge\" border=\"0\" height=\"200\">\n","\n","\n","Y aquí podéis verla en función de grafo:\n","\n","<img src=\"https://image.ibb.co/j258ud/rnn_graph.png\" alt=\"rnn_graph\" border=\"0\" width=\"600\">\n","\n","Y así es como funcionan!!\n","\n","Todo esto está sacado de este video en inglés que os recomiendo encarecidamente que veais tantas veces como sea necesario para asimilar y asentar lo que os acabo de explicar: https://www.youtube.com/watch?v=UNmqTiOnRfg\n"]},{"cell_type":"markdown","metadata":{"id":"DckLwzz-cXuR","colab_type":"text"},"source":["### Y para qué se usan las RNN?\n","\n","Pues existen varios tipos:\n","\n","<img src=\"https://image.ibb.co/cRTptJ/rnn_types.png\" alt=\"rnn_types\" border=\"0\">\n","\n","Son muy buenas sobretodo cuando nuestros datos son secuenciales:\n","\n","* Predicción de acciones en bolsa\n"," * Los valores de una acción dependen en gran medida de los valores que tenía anteriormente\n","* Generación de secuencias\n"," * Siempre que nuestros datos sean secuencias y un dato en un instante $t$ dependa del dato en el instante $t-1$\n","* Generación de texto\n"," * Por ejemplo, cuando el movil te sugiere palabras. Se fija en la ultima palabra que has escrito, y en las letras que estás escribiendo en ese momento para sugerirte las próximas letras o incluso palabras\n","* Reconocimiento de voz\n"," * En este caso tenemos la anterior palabra reconocida, y el audio que nos llega en ese momento\n"," "]},{"cell_type":"markdown","metadata":{"id":"7PL_ZGJVqbna","colab_type":"text"},"source":["## 7.2 Long Short-Term Memory networks\n","\n","Ahora que ya sabéis cómo funcionan las redes recurrentes, vamos a ver las más famosas, las LSTM. Esta es la estructura de una RNN:\n","\n","<img src=\"https://image.ibb.co/mUUR7d/rnn_1.png\" alt=\"rnn_1\" border=\"0\">\n","\n","Pero antes, por qué son las más usadas? \n","\n","Resulta que las RNN convencionales tienen problemas de memoria. Paradójico, no creéis? Que las redes especialemente diseñadas para recordar sean incapaces de recordar a largo plazo. Y por qué esto es un problema?\n","\n","Pues volviendo al problema de nuestro roommate, para este ejemplo solo necesitamos conocer lo que comimos ayer, así que no pasaría nada. \n","\n","<img src=\"https://image.ibb.co/n6kNfy/rnn_2.png\" alt=\"rnn_2\" border=\"0\">\n","\n","Pero imagináos que en vez de un menú con 3 comidas, tuviese 60 platos.\n","\n","<img src=\"https://image.ibb.co/mCWv0y/rnn_3.png\" alt=\"rnn_3\" border=\"0\">\n","\n","Las RNN convencionales no serían capaces de recordar cosas que pasaron hace mucho tiempo. Sin embargo, las LSTM sí!\n","\n","Y por qué?\n","<img src=\"https://thumbs.gfycat.com/HarmfulJadedEquine-size_restricted.gif\" height=\"200\">\n","\n","Echémosle un ojo a la arquitectura de las RNN y de las LSTM:\n","\n","### RNN\n","\n","<img src=\"https://image.ibb.co/mkAind/rnn_rnn_arch.png\" alt=\"rnn_rnn_arch\" border=\"0\">\n","\n","### LSTM\n","\n","<img src=\"https://image.ibb.co/hw4LYJ/rnn_lstm_arch.png\" alt=\"rnn_lstm_arch\" border=\"0\">\n","\n","Resulta que donde las RNN tienen una sola capa, las LSTM tienen una combinación de capas que interactúan entre ellas de una forma muy especial.\n","\n","Vamos a tratar de entenderlo, pero primero, os explico la nomenclatura:\n","\n","<img src=\"https://image.ibb.co/na6W7d/rnn_arch_nomenclature.png\" alt=\"rnn_arch_nomenclature\" border=\"0\" height=\"100\">\n","\n","En los diagramas de arriba, por cada línea viaja un vector, desde la salida de un nodo hasta las entradas de otros. Los círculos rosas indican operaciones elemento a elemento, como sumas de vectores, mientras que las cajas amarillas son capas neuronales que se aprenden al entrenar. Las líneas que se unen indican concatenar, y las que se separan, que el mismo contenido de la linea viaja a dos destinos distintos.\n","\n","### La idea clave de las LSTMs\n","\n","La clave es el estado de la celda, que está indicado en el diagrama como la línea que viaja por la parte de arriba: \n","\n","<img src=\"https://image.ibb.co/hvJYLy/rnn_arch_cell_status.png\" alt=\"rnn_arch_cell_status\" border=\"0\">\n","\n","El estado de la celda es como una especie de cinta transportadora que viaja a lo largo de toda la arquitectura de la red con muy pocas interacciones (y las pocaas que tiene son lineales): lo cual implica que la información simplemente fluye sin ser modificada!\n","\n","La parte ingeniosa es que las capas de la LSTM pueden (o no) aportar información a esta cinta transportadora, y esa decisión la toman las \"puertas\":\n","\n","<img src=\"https://image.ibb.co/kjHSfy/rnn_arch_gate.png\" alt=\"rnn_arch_gate\" border=\"0\">\n","\n","Las puertas no son otra cosa que una forma de regular cuidadosamente la información que llega a la cinta transportadora. Están compuestas de una red neuronal con una activación de tipo sigmoide y una multiplicación elemento a elemento.\n","\n","Así, la capa sigmoide da como salida un número entre 0 y uno, que implica cómo de importante es esa información para dejarla pasr a la cinta transportadora. Un 0 significa que no me importa, y un uno significa que es muy importante.\n","\n","Como podéis ver en el diagrama, una LSTM tiene 3 puertas de este tipo, para proteger y controlar la cina transportadora.\n","\n","Por temas de tiempo no puedo meterme a mirar en detalle su funcionamiento, pero podéis verlo genial explicado aquí: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","Y este blog también es muy interesante, para vosotros, curiosos: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","\n","**Visto esto, vamos a ver qué es lo que pueden hacer las Redes Recurrentes!**\n","\n","APLICACIONES: https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IX-32-mNGlgW","colab_type":"text"},"source":["### Ejemplo de LSTM con imágenes\n","\n","Vamos a ver si podemos clasificar las imágenes del MNIST con una LSTM, y con qué precisión:"]},{"cell_type":"code","metadata":{"id":"dH-B2Hu_x-a6","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kQESzO8Gpp3","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from keras.datasets import mnist\n","from keras.utils import np_utils\n","from keras import initializers\n","  \n","# Hyper parameters\n","batch_size = 128\n","nb_epoch = 10\n","\n","# Parameters for MNIST dataset\n","img_rows, img_cols = 28, 28\n","nb_classes = 10\n","\n","# Parameters for LSTM network\n","nb_lstm_outputs = 30\n","nb_time_steps = img_rows\n","dim_input_vector = img_cols\n","\n","# Load MNIST dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","print('X_train original shape:', X_train.shape)\n","input_shape = (nb_time_steps, dim_input_vector)\n","\n","X_train = X_train.astype('float32') / 255.\n","X_test = X_test.astype('float32') / 255.\n","\n","Y_train = np_utils.to_categorical(y_train, nb_classes)\n","Y_test = np_utils.to_categorical(y_test, nb_classes)\n","\n","print('X_train shape:', X_train.shape)\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fj3Wd_h9GyZL","colab_type":"code","colab":{}},"source":["# Construimos la LSTM\n","\n","# COMPLETAR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5OK7xlWGzy6","colab_type":"code","colab":{}},"source":["# Entrenamos\n","\n","# COMPLETAR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EftAupsOHHEi","colab_type":"code","colab":{}},"source":["# Evaluamos\n","evaluation = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1)\n","print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sujpiHLLNgLB","colab_type":"code","colab":{}},"source":["# Fuente: https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o95JMK1Ijcxd","colab_type":"text"},"source":["### Ejemplo de LSTM que predice qué letra viene después en el alfabeto"]},{"cell_type":"code","metadata":{"id":"YRO44I4sjg9i","colab_type":"code","colab":{}},"source":["# Naive LSTM to learn one-char to one-char mapping (https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\n","import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.utils import np_utils\n","\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","# define the raw dataset\n","alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n","# create mapping of characters to integers (0-25) and the reverse\n","char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n","int_to_char = dict((i, c) for i, c in enumerate(alphabet))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cmtxu_9f3kgt","colab_type":"code","colab":{}},"source":["# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 1\n","dataX = []\n","dataY = []\n","for i in range(0, len(alphabet) - seq_length, 1):\n","\t\n","\t# COMPLETAR\n","\t\n","\tprint(seq_in, '->', seq_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IoFMoJ303ohA","colab_type":"code","colab":{}},"source":["# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n","\n","# normalize\n","X = X / float(len(alphabet))\n","\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjRfLQx33qdc","colab_type":"code","colab":{}},"source":["# create and fit the model\n","\n","# COMPLETAR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7n7qGiX3sfp","colab_type":"code","colab":{}},"source":["# summarize performance of the model\n","scores = model.evaluate(X, y, verbose=0)\n","print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","# demonstrate some model predictions\n","for pattern in dataX:\n","\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n","\tx = x / float(len(alphabet))\n","\tprediction = model.predict(x, verbose=0)\n","\tindex = numpy.argmax(prediction)\n","\tresult = int_to_char[index]\n","\tseq_in = [int_to_char[value] for value in pattern]\n","\tprint(seq_in, \"->\", result)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NO_UiqtWkxXh","colab_type":"text"},"source":["###Ejemplo de LSTM que predice qué letra viene después en el alfabeto teniendo como input grupos de 3 letras\n"]},{"cell_type":"code","metadata":{"id":"UQr4cr-Akt0p","colab_type":"code","colab":{}},"source":["# PEGAR CÓDIGO ANTERIOR"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPmo9dUbTY5p","colab_type":"text"},"source":["### Ejemplo de Time Series prediction con LSTMs\n","\n","Vamos a tratar de predecir el número viajeros de una aerolinea."]},{"cell_type":"code","metadata":{"id":"_dgLCBBnqbrA","colab_type":"code","colab":{}},"source":["# LSTM for international airline passengers problem with regression framing\n","# https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n","!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/airline/international-airline-passengers.csv\n","\n","import numpy\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","import math\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","\n","# convert an array of values into a dataset matrix\n","def create_dataset(dataset, look_back=1):\n","\t\n","\t# COMPLETAR \n","\t\n","\treturn numpy.array(dataX), numpy.array(dataY)\n","\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","\n","# load the dataset\n","dataframe = read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n","dataset = dataframe.values\n","dataset = dataset.astype('float32')\n","\n","# normalize the dataset\n","\n","# COMPLETAR\n","\n","# split into train and test sets\n","train_size = int(len(dataset) * 0.67)\n","test_size = len(dataset) - train_size\n","train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n","\n","# reshape into X=t and Y=t+1\n","look_back = 1\n","trainX, trainY = create_dataset(train, look_back)\n","testX, testY = create_dataset(test, look_back)\n","\n","# reshape input to be [samples, time steps, features]\n","trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n","testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n","\n","# create and fit the LSTM network\n","model = Sequential()\n","model.add(LSTM(4, input_shape=(1, look_back)))\n","model.add(Dense(1))\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n","\n","# make predictions\n","trainPredict = model.predict(trainX)\n","testPredict = model.predict(testX)\n","\n","# invert predictions\n","\n","# COMPLETAR\n","\n","# calculate root mean squared error\n","trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))\n","\n","# shift train predictions for plotting\n","trainPredictPlot = numpy.empty_like(dataset)\n","trainPredictPlot[:, :] = numpy.nan\n","trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n","\n","# shift test predictions for plotting\n","testPredictPlot = numpy.empty_like(dataset)\n","testPredictPlot[:, :] = numpy.nan\n","testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n","\n","# plot baseline and predictions\n","plt.plot(scaler.inverse_transform(dataset))\n","plt.plot(trainPredictPlot)\n","plt.plot(testPredictPlot)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIZShF-PTdFp","colab_type":"text"},"source":["### Ejemplo de Time Series prediction con LSTMs\n","\n","Vamos a tratar de solucionar el problema del IMDB."]},{"cell_type":"code","metadata":{"id":"Iii7Rca2orFW","colab_type":"code","colab":{}},"source":["# imdb problem\n","# https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n","  \n","# LSTM for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","\n","# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, epochs=3, batch_size=64)\n","\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V_Tlzmh31gDl","colab_type":"text"},"source":["### Ejemplo de Time Series prediction con LSTMs\n","\n","Vamos a tratar de predecir el número de ventas de un champú. Para ello, disponemos de un dataset en el que se incluyen las ventas de los últimos 3 años."]},{"cell_type":"code","metadata":{"id":"54oMSRo74Kow","colab_type":"code","colab":{}},"source":["!rm shampo*\n","!ls -la"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlRG2K783dpK","colab_type":"code","colab":{}},"source":["!wget -O shampoo-sales.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n","!ls -la"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"15mpWN8JjlJZ","colab_type":"code","colab":{}},"source":["from pandas import DataFrame\n","from pandas import Series\n","from pandas import concat\n","from pandas import read_csv\n","from pandas import datetime\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from math import sqrt\n","from matplotlib import pyplot\n","import numpy\n","\n","# date-time parsing function for loading the dataset\n","def parser(x):\n","\treturn datetime.strptime('190'+x, '%Y-%m')\n","\n","# frame a sequence as a supervised learning problem\n","def timeseries_to_supervised(data, lag=1):\n","\tdf = DataFrame(data)\n","\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n","\tcolumns.append(df)\n","\tdf = concat(columns, axis=1)\n","\tdf.fillna(0, inplace=True)\n","\treturn df\n","\n","# create a differenced series\n","def difference(dataset, interval=1):\n","\tdiff = list()\n","\tfor i in range(interval, len(dataset)):\n","\t\tvalue = dataset[i] - dataset[i - interval]\n","\t\tdiff.append(value)\n","\treturn Series(diff)\n","\n","# invert differenced value\n","def inverse_difference(history, yhat, interval=1):\n","\treturn yhat + history[-interval]\n","\n","# scale train and test data to [-1, 1]\n","def scale(train, test):\n","\t# fit scaler\n","\tscaler = MinMaxScaler(feature_range=(-1, 1))\n","\tscaler = scaler.fit(train)\n","\t# transform train\n","\ttrain = train.reshape(train.shape[0], train.shape[1])\n","\ttrain_scaled = scaler.transform(train)\n","\t# transform test\n","\ttest = test.reshape(test.shape[0], test.shape[1])\n","\ttest_scaled = scaler.transform(test)\n","\treturn scaler, train_scaled, test_scaled\n","\n","# inverse scaling for a forecasted value\n","def invert_scale(scaler, X, value):\n","\tnew_row = [x for x in X] + [value]\n","\tarray = numpy.array(new_row)\n","\tarray = array.reshape(1, len(array))\n","\tinverted = scaler.inverse_transform(array)\n","\treturn inverted[0, -1]\n","\n","# fit an LSTM network to training data\n","def fit_lstm(train, batch_size, nb_epoch, neurons):\n","\tX, y = train[:, 0:-1], train[:, -1]\n","\tX = X.reshape(X.shape[0], 1, X.shape[1])\n","\tmodel = Sequential()\n","\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n","\tmodel.add(Dense(1))\n","\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n","\tfor i in range(nb_epoch):\n","\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n","\t\tmodel.reset_states()\n","\treturn model\n","\n","# make a one-step forecast\n","def forecast_lstm(model, batch_size, X):\n","\tX = X.reshape(1, 1, len(X))\n","\tyhat = model.predict(X, batch_size=batch_size)\n","\treturn yhat[0,0]\n","\n","# load dataset\n","series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n","print('Dataset cargado')\n","\n","# transform data to be stationary\n","raw_values = series.values\n","diff_values = difference(raw_values, 1)\n","print('Datos transformados')\n","\n","# transform data to be supervised learning\n","supervised = timeseries_to_supervised(diff_values, 1)\n","supervised_values = supervised.values\n","print('Datos transformados a supervisado')\n","\n","# split data into train and test-sets\n","train, test = supervised_values[0:-12], supervised_values[-12:]\n","\n","# transform the scale of the data\n","scaler, train_scaled, test_scaled = scale(train, test)\n","print('Datos escalados')\n","\n","# fit the model\n","lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n","# forecast the entire training dataset to build up state for forecasting\n","train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n","lstm_model.predict(train_reshaped, batch_size=1)\n","\n","# walk-forward validation on the test data\n","predictions = list()\n","for i in range(len(test_scaled)):\n","\t# make one-step forecast\n","\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n","\tyhat = forecast_lstm(lstm_model, 1, X)\n","\t# invert scaling\n","\tyhat = invert_scale(scaler, X, yhat)\n","\t# invert differencing\n","\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n","\t# store forecast\n","\tpredictions.append(yhat)\n","\texpected = raw_values[len(train) + i + 1]\n","\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n","\n","# report performance\n","rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n","print('Test RMSE: %.3f' % rmse)\n","# line plot of observed vs predicted\n","pyplot.plot(raw_values[-12:])\n","pyplot.plot(predictions)\n","pyplot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8OIf8z6HzAD","colab_type":"text"},"source":["### Ejemplo generación de música"]},{"cell_type":"code","metadata":{"id":"t4l8nxP7H1Pc","colab_type":"code","colab":{}},"source":["!git clone https://github.com/Skuldur/Classical-Piano-Composer\n","!ls -la"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfP7LkprH30K","colab_type":"code","colab":{}},"source":["# FUENTE: https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\n","\n","\"\"\" This module prepares midi file data and feeds it to the neural\n","    network for training \"\"\"\n","import glob\n","import pickle\n","import numpy\n","from music21 import converter, instrument, note, chord, stream\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import Activation\n","from keras.utils import np_utils\n","from keras.callbacks import ModelCheckpoint\n","\n","def train_network():\n","    \"\"\" Train a Neural Network to generate music \"\"\"\n","    notes = get_notes()\n","\n","    # get amount of pitch names\n","    n_vocab = len(set(notes))\n","\n","    network_input, network_output = prepare_sequences(notes, n_vocab)\n","\n","    model = create_network(network_input, n_vocab)\n","\n","    train(model, network_input, network_output)\n","\n","def get_notes():\n","    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n","    notes = []\n","\n","    for file in glob.glob(\"Classical-Piano-Composer/midi_songs/*.mid\"):\n","        midi = converter.parse(file)\n","\n","        print(\"Parsing %s\" % file)\n","\n","        notes_to_parse = None\n","\n","        try: # file has instrument parts\n","            s2 = instrument.partitionByInstrument(midi)\n","            notes_to_parse = s2.parts[0].recurse() \n","        except: # file has notes in a flat structure\n","            notes_to_parse = midi.flat.notes\n","\n","        for element in notes_to_parse:\n","            if isinstance(element, note.Note):\n","                notes.append(str(element.pitch))\n","            elif isinstance(element, chord.Chord):\n","                notes.append('.'.join(str(n) for n in element.normalOrder))\n","\n","    with open('Classical-Piano-Composer/data/notes', 'wb') as filepath:\n","        pickle.dump(notes, filepath)\n","\n","    return notes\n","\n","def prepare_sequences(notes, n_vocab):\n","    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n","    sequence_length = 100\n","\n","    # get all pitch names\n","    pitchnames = sorted(set(item for item in notes))\n","\n","     # create a dictionary to map pitches to integers\n","    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n","\n","    network_input = []\n","    network_output = []\n","\n","    # create input sequences and the corresponding outputs\n","    for i in range(0, len(notes) - sequence_length, 1):\n","        sequence_in = notes[i:i + sequence_length]\n","        sequence_out = notes[i + sequence_length]\n","        network_input.append([note_to_int[char] for char in sequence_in])\n","        network_output.append(note_to_int[sequence_out])\n","\n","    n_patterns = len(network_input)\n","\n","    # reshape the input into a format compatible with LSTM layers\n","    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n","    # normalize input\n","    network_input = network_input / float(n_vocab)\n","\n","    network_output = np_utils.to_categorical(network_output)\n","\n","    return (network_input, network_output)\n","\n","def create_network(network_input, n_vocab):\n","    \"\"\" create the structure of the neural network \"\"\"\n","    model = Sequential()\n","    model.add(LSTM(\n","        512,\n","        input_shape=(network_input.shape[1], network_input.shape[2]),\n","        return_sequences=True\n","    ))\n","    model.add(Dropout(0.3))\n","    model.add(LSTM(512, return_sequences=True))\n","    model.add(Dropout(0.3))\n","    model.add(LSTM(512))\n","    model.add(Dense(256))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(n_vocab))\n","    model.add(Activation('softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","\n","    return model\n","\n","def train(model, network_input, network_output):\n","    \"\"\" train the neural network \"\"\"\n","    filepath = \"Classical-Piano-Composer/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n","    checkpoint = ModelCheckpoint(\n","        filepath,\n","        monitor='loss',\n","        verbose=0,\n","        save_best_only=True,\n","        mode='min'\n","    )\n","    callbacks_list = [checkpoint]\n","\n","    model.fit(network_input, network_output, epochs=1, batch_size=64, callbacks=callbacks_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuQF5vaDINDe","colab_type":"code","colab":{}},"source":["train_network()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugtVjpA8PD5s","colab_type":"code","colab":{}},"source":["# vamos ahora a ver la mejora de velocidad que obtenemos utilizando CuDNNLSTM\n","from keras.layers import CuDNNLSTM\n","\n","def create_network(network_input, n_vocab):\n","    \"\"\" create the structure of the neural network \"\"\"\n","    model = Sequential()\n","    model.add(CuDNNLSTM(\n","        512,\n","        input_shape=(network_input.shape[1], network_input.shape[2]),\n","        return_sequences=True\n","    ))\n","    model.add(Dropout(0.3))\n","    model.add(CuDNNLSTM(512, return_sequences=True))\n","    model.add(Dropout(0.3))\n","    model.add(CuDNNLSTM(512))\n","    model.add(Dense(256))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(n_vocab))\n","    model.add(Activation('softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","\n","    return model\n","  \n","def train(model, network_input, network_output):\n","    \"\"\" train the neural network \"\"\"\n","    filepath = \"Classical-Piano-Composer/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n","    checkpoint = ModelCheckpoint(\n","        filepath,\n","        monitor='loss',\n","        verbose=0,\n","        save_best_only=True,\n","        mode='min'\n","    )\n","    callbacks_list = [checkpoint]\n","\n","    model.fit(network_input, network_output, epochs=10, batch_size=64, callbacks=callbacks_list)\n","  \n","train_network()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Zl2tYSwTqwF","colab_type":"code","colab":{}},"source":["!ls -la Classical-Piano-Composer/*.hdf5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOqY7da9exoR","colab_type":"code","colab":{}},"source":["weights_path = ## ELEGIR CHECKPOINT ## Ejemplo: 'Classical-Piano-Composer/weights-improvement-04-4.7081-bigger.hdf5'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGfP38GKIVqw","colab_type":"code","colab":{}},"source":["from tqdm import tqdm \n","\n","def generate():\n","    \"\"\" Generate a piano midi file \"\"\"\n","    #load the notes used to train the model\n","    with open('Classical-Piano-Composer/data/notes', 'rb') as filepath:\n","        notes = pickle.load(filepath)\n","\n","    # Get all pitch names\n","    pitchnames = sorted(set(item for item in notes))\n","    # Get all pitch names\n","    n_vocab = len(set(notes))\n","\n","    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n","    model = create_network(normalized_input, n_vocab)\n","    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n","    output_midi = create_midi(prediction_output)\n","    return output_midi\n","    \n","def prepare_sequences(notes, pitchnames, n_vocab):\n","    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n","    # map between notes and integers and back\n","    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n","\n","    sequence_length = 100\n","    network_input = []\n","    output = []\n","    for i in range(0, len(notes) - sequence_length, 1):\n","        sequence_in = notes[i:i + sequence_length]\n","        sequence_out = notes[i + sequence_length]\n","        network_input.append([note_to_int[char] for char in sequence_in])\n","        output.append(note_to_int[sequence_out])\n","\n","    n_patterns = len(network_input)\n","\n","    # reshape the input into a format compatible with LSTM layers\n","    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n","    # normalize input\n","    normalized_input = normalized_input / float(n_vocab)\n","\n","    return (network_input, normalized_input)\n","  \n","def create_network(network_input, n_vocab):\n","    \"\"\" create the structure of the neural network \"\"\"\n","    model = Sequential()\n","    model.add(LSTM(\n","        512,\n","        input_shape=(network_input.shape[1], network_input.shape[2]),\n","        return_sequences=True\n","    ))\n","    model.add(Dropout(0.3))\n","    model.add(LSTM(512, return_sequences=True))\n","    model.add(Dropout(0.3))\n","    model.add(LSTM(512))\n","    model.add(Dense(256))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(n_vocab))\n","    model.add(Activation('softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","\n","    # Load the weights to each node\n","    model.load_weights(weights_path)\n","\n","    return model\n","\n","def generate_notes(model, network_input, pitchnames, n_vocab):\n","    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n","    # pick a random sequence from the input as a starting point for the prediction\n","    start = numpy.random.randint(0, len(network_input)-1)\n","\n","    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n","\n","    pattern = network_input[start]\n","    prediction_output = []\n","\n","    # generate 500 notes\n","    for note_index in tqdm(range(500)):\n","        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n","        prediction_input = prediction_input / float(n_vocab)\n","\n","        prediction = model.predict(prediction_input, verbose=0)\n","\n","        index = numpy.argmax(prediction)\n","        result = int_to_note[index]\n","        prediction_output.append(result)\n","\n","        pattern.append(index)\n","        pattern = pattern[1:len(pattern)]\n","\n","    return prediction_output\n","\n","def create_midi(prediction_output):\n","    \"\"\" convert the output from the prediction to notes and create a midi file\n","        from the notes \"\"\"\n","    offset = 0\n","    output_notes = []\n","\n","    # create note and chord objects based on the values generated by the model\n","    for pattern in prediction_output:\n","        # pattern is a chord\n","        if ('.' in pattern) or pattern.isdigit():\n","            notes_in_chord = pattern.split('.')\n","            notes = []\n","            for current_note in notes_in_chord:\n","                new_note = note.Note(int(current_note))\n","                new_note.storedInstrument = instrument.Piano()\n","                notes.append(new_note)\n","            new_chord = chord.Chord(notes)\n","            new_chord.offset = offset\n","            output_notes.append(new_chord)\n","        # pattern is a note\n","        else:\n","            new_note = note.Note(pattern)\n","            new_note.offset = offset\n","            new_note.storedInstrument = instrument.Piano()\n","            output_notes.append(new_note)\n","\n","        # increase offset each iteration so that notes do not stack\n","        offset += 0.5\n","\n","    midi_stream = stream.Stream(output_notes)\n","\n","    midi_stream.write('midi', fp='Classical-Piano-Composer/test_output.mid')\n","    \n","    return output_notes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyBZ3SzEIkFm","colab_type":"code","colab":{}},"source":["output_notes = generate()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCpcVvStYYMN","colab_type":"code","colab":{}},"source":["output_notes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sA3fcLBeR-N9","colab_type":"code","colab":{}},"source":["!ls -la Classical-Piano-Composer/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LwcpURqVMPB","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download('Classical-Piano-Composer/test_output.mid')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_-TxJQrnmDa","colab_type":"text"},"source":["Para escuchar MIDIs creados con una red entrenada: https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5"]},{"cell_type":"markdown","metadata":{"id":"aL9sFwLqTkI8","colab_type":"text"},"source":["Y aquí tenéis algunos otros ejemplos muy interesantes:"]},{"cell_type":"code","metadata":{"id":"H4wCC0vt3PPV","colab_type":"code","colab":{}},"source":["# Ejemplo de trading\n","# https://github.com/happynoom/DeepTrade_keras\n","\n","# Ejemplo de cómo generar el 6o libro de GOT\n","# https://github.com/zackthoutt/got-book-6\n","\n","# Ejemplo de detección de anuncios en videos\n","# https://github.com/harvitronix/continuous-online-video-classification-blog"],"execution_count":0,"outputs":[]}]}